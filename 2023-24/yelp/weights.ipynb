{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore different options for document vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yelp.loader import load_sample, stars\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 3_000\n",
    "data = list(stars(load_sample()))[:limit]\n",
    "documents, y = list(zip(*data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 (SpaCy)\n",
    "- simple string cleaning\n",
    "- lowercase\n",
    "- lemmatization\n",
    "- POS filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys \n",
    "sys.path.append('../nlp')\n",
    "from nlp.vectorize import spacy_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = documents[10]\n",
    "print(text)\n",
    "print(spacy_tokenizer(nlp, text, lowercase=True, lemma=True, pos_filter=['PUNCT', 'DET']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer\n",
    "From [sklearn CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.vectorize import spacy_count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, vectorizer = spacy_count_vectorizer(nlp, documents, lowercase=True, lemma=True, pos_filter=['PUNCT', 'DET'], min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "print(len(features))\n",
    "print(features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf = pd.DataFrame(X.toarray(), columns=features)\n",
    "Xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = 10\n",
    "print(documents[test_doc])\n",
    "print([(w, score) for w, score in Xdf.iloc[test_doc].sort_values(ascending=False).head(20).items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 (word_tokenizer)\n",
    "- simple string cleaning\n",
    "- lowercase\n",
    "- no lemmatization\n",
    "- no POS filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tokenizer = lambda x: [w.lower() for w in word_tokenize(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=simple_tokenizer, token_pattern=None, min_df=2)\n",
    "W = vectorizer.fit_transform(documents)\n",
    "Wdf = pd.DataFrame(W.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features exploration\n",
    "\n",
    "Let's define the notion of document frequency as:\n",
    "$$\n",
    "df(w) = \\mid \\{d: w \\in d\\} \\mid\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Wdf.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df / Wdf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(Wdf.shape[0] / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot words in the space of documents by showing their DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "W2d = tsne.fit_transform(Wdf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_df_words_indexes = [i for i, (k, w) in enumerate(df.items()) if w > 300]\n",
    "low_df_words_indexes = [i for i, (k, w) in enumerate(df.items()) if 20 < w < 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8), ncols=3, nrows=2)\n",
    "sns.scatterplot(x=W2d[:,0], y=W2d[:,1], ax=ax[0, 0], alpha=.2, hue=df, palette=\"rocket\", size=df)\n",
    "sns.scatterplot(x=W2d[high_df_words_indexes,0], y=W2d[high_df_words_indexes,1], \n",
    "                ax=ax[0, 1], alpha=.2, hue=df.values[high_df_words_indexes], palette=\"rocket\", size=df.values[high_df_words_indexes])\n",
    "sns.scatterplot(x=W2d[low_df_words_indexes,0], y=W2d[low_df_words_indexes,1], \n",
    "                ax=ax[0, 2], alpha=.2, hue=df.values[low_df_words_indexes], palette=\"rocket\", size=df.values[low_df_words_indexes])\n",
    "ax[0, 0].set_title('All words')\n",
    "ax[0, 1].set_title('DF > 300')\n",
    "ax[0, 2].set_title('50 < DF < 300')\n",
    "leg = ax[0, 0].get_legend()\n",
    "leg.set_title(\"\")\n",
    "\n",
    "sns.lineplot(x=range(df.shape[0]), y=df.sort_values(ascending=False).values, ax=ax[1, 0], color='#cc0000')\n",
    "sns.lineplot(x=range(df.shape[0]), y=df.sort_values(ascending=False).values, ax=ax[1, 1], color='#cc0000')\n",
    "sns.scatterplot(x=Wdf.mean(axis=0), y=df, ax=ax[1, 2], alpha=.6, color='#cc0000')\n",
    "ax[1, 1].set_yscale('log')\n",
    "ax[1, 0].set_xlabel('Words')\n",
    "ax[1, 0].set_ylabel('DF')\n",
    "ax[1, 1].set_xlabel('Words')\n",
    "ax[1, 1].set_ylabel('DF (log)')\n",
    "ax[1, 2].set_xlabel('Occurrences per document (mean)')\n",
    "ax[1, 2].set_ylabel('DF')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = 10\n",
    "print(documents[test_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wdf.iloc[test_document].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for w, s in Wdf.iloc[test_document].items():\n",
    "    if s > 0:\n",
    "        data[w] = s * idf[w]\n",
    "tfidf = pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count VS TdfIdf vectorizers\n",
    "Lets' check the different effect of **TF** VS **TfIdf** on the classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yelp.loader import load_sample, stars\n",
    "import pandas as pd\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10_000\n",
    "data = list(stars(load_sample()))[:limit]\n",
    "documents, y = list(zip(*data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    t = re.sub(\"\\s\\s+\" , \" \", x)\n",
    "    t = re.sub(\"[\\n]+\", \" \", t)\n",
    "    t = re.sub(\"[\\r\\n]+\", \" \", t)\n",
    "    return word_tokenize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=tokenizer, token_pattern=None, min_df=2)\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer, token_pattern=None, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = count_vectorizer.fit_transform(documents)\n",
    "T = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C.shape, T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train, C_test, c_train, c_test = train_test_split(C, y)\n",
    "T_train, T_test, t_train, t_test = train_test_split(T, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification test\n",
    "We use **Born** in order to explore feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bornrule import BornClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "born_c = BornClassifier()\n",
    "born_c.fit(C_train, c_train)\n",
    "c_pred = born_c.predict(C_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "born_t = BornClassifier()\n",
    "born_t.fit(T_train, t_train)\n",
    "t_pred = born_t.predict(T_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.81      0.71       362\n",
      "           2       0.41      0.22      0.29       202\n",
      "           3       0.48      0.20      0.28       294\n",
      "           4       0.53      0.27      0.35       579\n",
      "           5       0.65      0.92      0.76      1063\n",
      "\n",
      "    accuracy                           0.61      2500\n",
      "   macro avg       0.54      0.48      0.48      2500\n",
      "weighted avg       0.58      0.61      0.56      2500\n",
      "\n",
      "TfIdf Vectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.83      0.69       337\n",
      "           2       0.40      0.24      0.30       222\n",
      "           3       0.36      0.22      0.27       286\n",
      "           4       0.48      0.34      0.40       604\n",
      "           5       0.68      0.83      0.75      1051\n",
      "\n",
      "    accuracy                           0.59      2500\n",
      "   macro avg       0.50      0.49      0.48      2500\n",
      "weighted avg       0.56      0.59      0.56      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Count Vectorizer')\n",
    "print(classification_report(c_test, c_pred, zero_division=0))\n",
    "print('TfIdf Vectorizer')\n",
    "print(classification_report(t_test, t_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = pd.DataFrame(born_c.explain().toarray(), index=count_vectorizer.get_feature_names_out(), columns=range(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.012075</td>\n",
       "      <td>0.007701</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>0.011959</td>\n",
       "      <td>0.017360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%</th>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&amp;</th>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.001060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>étions</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>étoiles</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>été</th>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>être</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.000881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>über</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15796 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1         2         3         4         5\n",
       "!        0.012075  0.007701  0.008109  0.011959  0.017360\n",
       "#        0.000547  0.000291  0.000574  0.000516  0.000532\n",
       "$        0.001977  0.002233  0.001842  0.001674  0.001215\n",
       "%        0.000384  0.000349  0.000269  0.000270  0.000357\n",
       "&        0.000793  0.000685  0.000765  0.000881  0.001060\n",
       "...           ...       ...       ...       ...       ...\n",
       "étions   0.000000  0.003869  0.000000  0.000000  0.000000\n",
       "étoiles  0.000000  0.004887  0.000000  0.000000  0.000000\n",
       "été      0.001286  0.000000  0.000000  0.001499  0.000761\n",
       "être     0.000000  0.000000  0.000000  0.001086  0.000881\n",
       "über     0.000000  0.003075  0.000000  0.000000  0.003978\n",
       "\n",
       "[15796 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E#.sort_values(5, ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mutual Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from collections import defaultdict\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens, list(nltk.ngrams(tokens, n=2))\n",
    "\n",
    "def counter(corpus):\n",
    "    unigram, bigram = defaultdict(lambda: 0), defaultdict(lambda: 0)\n",
    "    for text in corpus:\n",
    "        tokens, bigrams_ = bigrams(text)\n",
    "        for token in tokens:\n",
    "            unigram[token] += 1\n",
    "        for bi in bigrams_:\n",
    "            bigram[bi] += 1\n",
    "    return unigram, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, B = counter(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = pd.Series(U)\n",
    "B = pd.Series(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = defaultdict(lambda: 0)\n",
    "b_total = B.sum()\n",
    "u_total = U.sum()\n",
    "for (x, y), count in B.items():\n",
    "    if count < 10 or count > 1000:\n",
    "        pass \n",
    "    else:\n",
    "        if U[x] > 2 and U[y] > 2:\n",
    "            p_b = count / b_total\n",
    "            p_x = U[x] / u_total\n",
    "            p_y = U[y] / u_total\n",
    "            mu[(x, y)] = p_b * np.log(p_b / (p_x * p_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = pd.Series(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ca', \"n't\") 0.003132475752508614\n",
      "('you', 'can') 0.0026713143708323147\n",
      "('as', 'well') 0.002647506096591929\n",
      "('a', 'little') 0.0025655714144917955\n",
      "('have', 'been') 0.002505590545834812\n",
      "('i', 'am') 0.002444293148099181\n",
      "('you', \"'re\") 0.0023766671837774766\n",
      "('a', 'few') 0.002336602034943331\n",
      "('customer', 'service') 0.0023290891306705246\n",
      "('they', 'were') 0.002293980461441209\n",
      "('a', 'lot') 0.0021172697221911153\n",
      "('they', 'have') 0.0020615338788516095\n",
      "('to', 'go') 0.0019992113238811752\n",
      "('we', 'had') 0.0019621656170151444\n",
      "('was', 'very') 0.0019598608811676878\n",
      "('highly', 'recommend') 0.0019119761592289318\n",
      "('when', 'i') 0.0018217573515197539\n",
      "('a', 'bit') 0.0017996985759788683\n",
      "(\"'ve\", 'been') 0.0017519001741248355\n",
      "('they', 'are') 0.0017466979337219112\n",
      "('will', 'be') 0.0017204536093814824\n",
      "('a', 'great') 0.0016446674696337692\n",
      "('ice', 'cream') 0.0016129710151046469\n",
      "('place', 'is') 0.001599425498537736\n",
      "('wo', \"n't\") 0.001596063603480355\n",
      "('first', 'time') 0.0015702445222478556\n",
      "('at', 'least') 0.0015491898412163508\n",
      "(',', 'which') 0.0015315185825575564\n",
      "('i', 'will') 0.0015005770255586517\n",
      "('lot', 'of') 0.0014994880528520075\n",
      "('to', 'try') 0.0014987087490521017\n",
      "('las', 'vegas') 0.001494009495583699\n",
      "('out', 'of') 0.0014794742279785375\n",
      "('my', 'husband') 0.0014409224892622113\n",
      "('want', 'to') 0.0014267292968276502\n",
      "('i', 'do') 0.0014220730335746004\n",
      "('i', 'could') 0.0013827068736830715\n",
      "('service', 'was') 0.0013707806803928992\n",
      "('my', 'favorite') 0.001368801965039718\n",
      "('i', 'love') 0.0013671202775470807\n",
      "('have', 'a') 0.0013558675168459642\n",
      "('going', 'to') 0.0013506690519755368\n",
      "('come', 'back') 0.0013466421604414395\n",
      "('there', 'are') 0.0013433637527798742\n",
      "('would', 'be') 0.0013387134540754951\n",
      "('i', 'did') 0.0013322762733134856\n",
      "('does', \"n't\") 0.0013210793497718575\n",
      "('food', 'was') 0.001309605854117242\n",
      "('the', 'staff') 0.0013093879440226582\n",
      "('the', 'same') 0.001293417774711355\n",
      "('food', 'is') 0.0012807513109627782\n",
      "('which', 'is') 0.0012795438522638794\n",
      "('go', 'back') 0.001259826388866426\n",
      "('was', \"n't\") 0.0012591032316311363\n",
      "('you', 'are') 0.0012415190646886333\n",
      "('so', 'i') 0.0012321098575631868\n",
      "('decided', 'to') 0.0012288455621668136\n",
      "('be', 'back') 0.0012273762698731545\n",
      "('i', 'got') 0.0012189794833464408\n",
      "('i', 'think') 0.0012090565903495006\n",
      "('more', 'than') 0.0012054083528410422\n",
      "('could', \"n't\") 0.001195263390635334\n",
      "(',', 'so') 0.0011889532886517555\n",
      "('from', 'the') 0.0011755537501954624\n",
      "('did', 'not') 0.0011745065232328835\n",
      "('able', 'to') 0.0011604539973178053\n",
      "('the', 'service') 0.0011474121868989414\n",
      "('the', 'menu') 0.0011383964148870918\n",
      "('to', 'see') 0.001137860791127025\n",
      "('i', 'went') 0.001131945221478249\n",
      "('looking', 'for') 0.0011267320881613314\n",
      "('very', 'good') 0.0011158100136059036\n",
      "('a', 'good') 0.0011153974786611866\n",
      "(\"'ve\", 'ever') 0.0011139901137994723\n",
      "('my', 'wife') 0.0010963082709653496\n",
      "('?', '?') 0.0010856712876147379\n",
      "('will', 'definitely') 0.0010829680305885323\n",
      "('happy', 'hour') 0.0010827390533126946\n",
      "('there', 'was') 0.0010784260912975068\n",
      "('i', 'ordered') 0.0010768640989344477\n",
      "('kind', 'of') 0.0010766489935959435\n",
      "('they', 'do') 0.0010690421906063244\n",
      "('but', 'it') 0.0010559632678732883\n",
      "('which', 'was') 0.0010549727505672507\n",
      "('ended', 'up') 0.0010520813997795682\n",
      "('on', 'a') 0.0010490911552148046\n",
      "('to', 'eat') 0.0010397820606046885\n",
      "('i', \"'d\") 0.0010348495218886926\n",
      "('would', 'have') 0.0010214346798392835\n",
      "('next', 'time') 0.0010204817523164673\n",
      "('i', \"'ll\") 0.0010168606374825354\n",
      "('in', 'vegas') 0.0010164437721523088\n",
      "('have', 'to') 0.0010060013684864894\n",
      "('you', 'want') 0.0010027649937599298\n",
      "('we', 'ordered') 0.0009943958462640282\n",
      "('that', \"'s\") 0.0009894467595931659\n",
      "('.', 'she') 0.0009834754760665828\n",
      "('.', 'he') 0.0009795174464274194\n",
      "('so', 'much') 0.000968853701882184\n",
      "('lots', 'of') 0.0009609212518965631\n"
     ]
    }
   ],
   "source": [
    "for b, m in MU.sort_values(ascending=False).head(100).items():\n",
    "    print(b, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9581"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B[('.', 'i')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
