{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence to sequence models up to 2017:\n",
    "- Recurrent Neural Network\n",
    "- Long Short Term Memory\n",
    "- Gated Recurrent Unit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src = \"https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc\" width = \"60%\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main problems:\n",
    "1. sequential input flow $\\rightarrow$ slow training and prediction\n",
    "2. many multiplications $\\rightarrow$ exploding/vanishing gradients $\\rightarrow$ loss of information + reduced window capacity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers (Vaswani et al., 2017) are sequence2sequence encoder-decoder models mapping an input $x \\in \\mathbb{R}^x$ to an output $y \\in \\mathbb{R}^y$.\\\n",
    "They implement a mechanism called attention, which allows both parallelization ($\\rightarrow$ faster training and prediction) and an infinitely large context window ($\\rightarrow$ no more loss of information)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://pytorch.org/tutorials/_images/transformer_architecture.jpg\" width=\"30%\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the code is taken from https://github.com/ajhalthor/Transformer-Neural-Network .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 60 #number of tokens in each sentence\n",
    "d_model = 6 #dimension of embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/word_embedding_0.svg\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabularies lookup tables and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "#loading vocabulary through spacy\n",
    "import spacy\n",
    "nlp_en = spacy.load('en_core_web_lg')\n",
    "nlp_it = spacy.load('it_core_news_lg')\n",
    "italian_vocab = [START_TOKEN, PADDING_TOKEN] + list(nlp_it.vocab.strings) + [END_TOKEN]\n",
    "english_vocab = [START_TOKEN, PADDING_TOKEN] + list(nlp_en.vocab.strings) + [END_TOKEN]\n",
    "\n",
    "#lookup tables\n",
    "index_to_italian = {k:v for k,v in enumerate(italian_vocab)}\n",
    "italian_to_index = {v:k for k,v in enumerate(italian_vocab)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocab)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(559485, 660403)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italian_to_index['procioni'], english_to_index['racoons']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these vocabularies we can build $\\texttt{nn.Embedding}$ layers, which behave as lookup tables. Indeed, they take as input an index and they retrieve the corresponging vector.\\\n",
    "The vector representation indicated the weighted matrix is initialized as random values and will be updated by backpropagation.\\\n",
    "We also specify the dimension for each embedding. In the original paper, $d_{model} = 512$; for simplicity, here is set to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for the word racoons: tensor([-1.0445, -2.0593, -0.4914, -1.3520,  0.0834,  0.9723],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Embedding for the word procioni: tensor([ 0.8912, -0.1354,  0.6910,  0.5355,  0.3882, -0.1405],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "english_embedding = nn.Embedding(len(english_vocab), d_model)\n",
    "italian_embedding = nn.Embedding(len(italian_vocab), d_model)\n",
    "\n",
    "en_w, it_w = 660403, 559485\n",
    "print(f\"Embedding for the word {index_to_english[en_w]}: {english_embedding(torch.tensor(en_w))}\")\n",
    "print(f\"Embedding for the word {index_to_italian[it_w]}: {italian_embedding(torch.tensor(it_w))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing and Batching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://www.statmt.org/europarl/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "1. Preprocess each sentence by removing '\\n', applying $\\texttt{lower()}$ \n",
    "2. Filter those sentences which are too long and/or have unknown words\n",
    "3. Take 10000 of those filtered sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions for checking sentence validity\n",
    "def is_valid_tokens(sentence_tokenized, vocab):\n",
    "    for token in sentence_tokenized:\n",
    "        if isinstance(token, str):\n",
    "            w = token \n",
    "        else:\n",
    "            w = token.text\n",
    "        if w not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_length(sentence_tokenized, max_sequence_length):\n",
    "    return len(sentence_tokenized) < (max_sequence_length)\n",
    "\n",
    "def filter_and_preprocess(sent, tokenizer, vocab, max_s_len):\n",
    "    tokenized = tokenizer(sent)\n",
    "    if is_valid_length(tokenized, max_s_len) and is_valid_tokens(tokenized, vocab):\n",
    "        return str(sent)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking only the first 10000 sentences meeting the requirements\n",
    "MAX_SENTENCES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk word_tokenizer to speed up the process\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_tokenizer = lambda text: word_tokenize(text, language='italian')\n",
    "en_tokenizer = lambda text: word_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english sentences...\n",
      "Reading italian sentences...\n",
      "10000\r"
     ]
    }
   ],
   "source": [
    "english_file = \"/Users/flint/Data/europarl/it-en/europarl-v7.it-en.en\"\n",
    "italian_file = \"/Users/flint/Data/europarl/it-en/europarl-v7.it-en.it\"\n",
    "\n",
    "en_vocab = set(english_vocab)\n",
    "it_vocab = set(italian_vocab)\n",
    "\n",
    "#loading corpus for training\n",
    "count = 0\n",
    "english_lines, italian_lines = [], []\n",
    "english_sentences, italian_sentences = [], []\n",
    "\n",
    "print('Reading english sentences...')\n",
    "with open(english_file,'rt') as f:\n",
    "    english_lines = f.readlines()\n",
    "\n",
    "print('Reading italian sentences...')   \n",
    "with open(italian_file,'rt') as f:\n",
    "    italian_lines = f.readlines()\n",
    "    \n",
    "for (sentence_en, sentence_it) in zip(english_lines, italian_lines):\n",
    "    if count < MAX_SENTENCES:\n",
    "        preprocessed_sent_en = filter_and_preprocess(sentence_en.lower()[:-1], \n",
    "                                                    en_tokenizer, \n",
    "                                                    en_vocab, \n",
    "                                                    max_sequence_len)\n",
    "        if preprocessed_sent_en:\n",
    "            preprocessed_sent_it = filter_and_preprocess(sentence_it.lower()[:-1], \n",
    "                                                it_tokenizer, \n",
    "                                                it_vocab, \n",
    "                                                max_sequence_len)\n",
    "            if preprocessed_sent_it: \n",
    "                english_sentences.append(preprocessed_sent_en)\n",
    "                italian_sentences.append(preprocessed_sent_it)\n",
    "                count += 1\n",
    "                print(count, end='\\r')\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resumption of the session',\n",
       " 'i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ripresa della sessione',\n",
       " 'dichiaro ripresa la sessione del parlamento europeo, interrotta venerdì 17 dicembre e rinnovo a tutti i miei migliori auguri nella speranza che abbiate trascorso delle buone vacanze.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italian_sentences[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a custom $\\texttt{Dataset}$ class to store our input pairs and to obtain a nice interface for doing batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we build a dataset class for our specific MT task\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, italian_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.italian_sentences = italian_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.italian_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is our dataset instance, which will work flawlessly with other pytorch modules\n",
    "dataset = TextDataset(english_sentences, italian_sentences)\n",
    "\n",
    "#saving the dataset\n",
    "torch.save(dataset, '/Users/flint/Data/europarl/it-en/dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('so parliament should send a message, since that is the wish of the vast majority.',\n",
       " 'il parlamento dovrebbe pertanto inviare un messaggio, come auspica la stragrande maggioranza dei deputati.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "dataset = torch.load('/Users/flint/Data/europarl/it-en/dataset.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to batch our data: we simply create a $\\texttt{DataLoader}$ instance to iterate through the dataset in batches of size 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also batch our data to make training faster\n",
    "batch_size = 3 \n",
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)\n",
    "batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resumption of the session',\n",
       "  'i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
       "  \"please rise, then, for this minute' s silence.\"),\n",
       " ('ripresa della sessione',\n",
       "  'dichiaro ripresa la sessione del parlamento europeo, interrotta venerdì 17 dicembre e rinnovo a tutti i miei migliori auguri nella speranza che abbiate trascorso delle buone vacanze.',\n",
       "  'vi invito pertanto ad alzarvi in piedi per osservare appunto un minuto di silenzio.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step, we replace each word with its corresponding index in the vocabulary lookup tables.\\\n",
    "Then, we add \\<START\\>, \\<END\\> and \\<PADDING\\> tokens accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we build our tokenizer, using the previously-made lookup tables\n",
    "def tokenize(sentence, language_to_index, lang_tokenizer, start_token=True, end_token=True):\n",
    "    sentence_word_ids = [language_to_index[token.text] for token in lang_tokenizer(sentence)]\n",
    "    if start_token:\n",
    "        sentence_word_ids.insert(0, language_to_index[START_TOKEN])\n",
    "    if end_token:\n",
    "        sentence_word_ids.append(language_to_index[END_TOKEN])\n",
    "    for _ in range(len(sentence_word_ids), max_sequence_len):\n",
    "        sentence_word_ids.append(language_to_index[PADDING_TOKEN])\n",
    "    return torch.tensor(sentence_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([611090, 494930, 394675, 546092, 721448, 660268,   4897, 776471,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('my favourite animal is the raccoon.', english_to_index, nlp_en.tokenizer, start_token=False, end_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0, 452149, 508323, 307925, 556355, 677483, 452149, 559484,   3016,\n",
       "        681836,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('il mio animale preferito è il procione.', italian_to_index, nlp_it.tokenizer, start_token=True, end_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenized, it_tokenized = [], []\n",
    "for sentence_num in range(batch_size):\n",
    "    eng_sentence, it_sentence = batch[0][sentence_num], batch[1][sentence_num]\n",
    "    eng_tokenized.append(tokenize(eng_sentence, english_to_index, nlp_en.tokenizer, start_token=False, end_token=True))\n",
    "    #start and end tokens are required for beginning and ending in the generation phase\n",
    "    it_tokenized.append(tokenize(it_sentence, italian_to_index, nlp_it.tokenizer, start_token=True, end_token=True)) \n",
    "eng_tokenized = torch.stack(eng_tokenized)\n",
    "it_tokenized = torch.stack(it_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0, 582931, 386197, 606005, 681836,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1],\n",
       "        [     0, 390276, 582931, 479202, 606005, 385670, 538666, 412693,   1125,\n",
       "         464576, 659121,  13442, 390172, 400642, 581859, 290097, 650364, 449638,\n",
       "         506537, 506751, 320905, 520299, 620428, 356586, 291150, 645799, 386283,\n",
       "         342076, 655743,   3016, 681836,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1],\n",
       "        [     0, 661075, 466388, 544691, 294987, 304342, 455772, 547169, 542869,\n",
       "         532897, 312488, 652757, 508293, 389703, 610277,   3016, 681836,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it_tokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting all together: fancy class for Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class SentenceEmbedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, lang_tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.language_tokenizer = lang_tokenizer\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
    "\n",
    "        def tokenize(sentence, start_token=True, end_token=True):\n",
    "            sentence_word_ids = [self.language_to_index[token.text] for token in self.language_tokenizer(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_ids.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_ids.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_ids), self.max_sequence_length):\n",
    "                sentence_word_ids.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_ids)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "            tokenized.append(tokenize(batch[sentence_num], start_token, end_token))\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(get_device())\n",
    "    \n",
    "    def forward(self, x, start_token = True, end_token=True): # sentence\n",
    "        x = self.batch_tokenize(x, start_token, end_token)\n",
    "        x = self.embedding(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = SentenceEmbedding(max_sequence_len, d_model, english_to_index, nlp_en.tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "english_batch = next(iterator)[0]\n",
    "print(f\"Input batch: {english_batch}\\nOutput embeddings shapes:{[embedding.size() for embedding in sentence_embedding(english_batch, start_token = False, end_token = True)]}\\nOuput embeddings: {sentence_embedding(english_batch)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for displaying purposes, we will lower \n",
    "#max_sequence_len to 10\n",
    "max_sequence_len = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PE(\\text{position}, 2i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg) \n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, 2i+1) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
    "$$\n",
    "where $i$ is the embedding dimension index, $position$ is the position of the word in the sentence, $d_{model}$ is the size of the embedding.\n",
    "- sin and cos allow for constrained values\n",
    "- sin and cos are periodic -> easier to attend to relative and distant positions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, whenever position is an odd number $2i+1$, its respective cosine function still considers $2i$ in its denominator formula, which corresponds to $(2i+1)-1 = 2i$. For this reason, we use $\\texttt{repeat\\_interleave()}$ function to repeat even elements of the $i$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.arange(0,d_model,2, dtype=torch.float).repeat_interleave(2)[:d_model]\n",
    "i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code computes $10000^\\frac{2i}{d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = torch.pow(10000, 2*i/d_model)\n",
    "denominator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code computes $\\text{position}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(max_sequence_len, dtype=torch.float).reshape(max_sequence_len, 1)\n",
    "position"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this code computes $\\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_cos_argument = position/denominator\n",
    "sin_cos_argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply the $\\sin$ function to even vector positions ($\\texttt{[:, 0::2]}$) and the $\\cos$ function to odd vector positions ($\\texttt{[:, 1::2]}$).\\\n",
    "This computation gives us the final positional encoding ($\\texttt{PE}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = torch.zeros(size = sin_cos_argument.shape)\n",
    "#even positions\n",
    "PE[:, 0::2] = torch.sin(sin_cos_argument[:, 0::2])\n",
    "#odd positions\n",
    "PE[:, 1::2] = torch.cos(sin_cos_argument[:, 1::2])\n",
    "PE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fancy class for Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        i = torch.arange(0,self.d_model,2, dtype=torch.float).repeat_interleave(2)[:self.d_model]\n",
    "        denominator = torch.pow(10000, 2*i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        sin_cos_argument = position/denominator\n",
    "        PE = torch.zeros(size = sin_cos_argument.shape)\n",
    "        PE[:, 0::2] = torch.sin(sin_cos_argument[:, 0::2])\n",
    "        PE[:, 1::2] = torch.cos(sin_cos_argument[:, 1::2])\n",
    "        return PE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embeddings + Positional Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will update the $\\texttt{SentenceEmbedding}$ class to output embeddings with encoded position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting max_sequence_len\n",
    "max_sequence_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, lang_tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.language_tokenizer = lang_tokenizer\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
    "\n",
    "        def tokenize(sentence, start_token=True, end_token=True):\n",
    "            sentence_word_ids = [self.language_to_index[token.text] for token in self.language_tokenizer(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_ids.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_ids.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_ids), self.max_sequence_length):\n",
    "                sentence_word_ids.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_ids)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "            tokenized.append(tokenize(batch[sentence_num], start_token, end_token))\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(get_device())\n",
    "    \n",
    "    def forward(self, x, start_token = True, end_token=True): # sentence\n",
    "        x = self.batch_tokenize(x, start_token, end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = SentenceEmbedding(max_sequence_len, d_model, english_to_index, nlp_en.tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "english_batch = next(iterator)[0]\n",
    "print(f\"Input batch: {english_batch}\\nOutput embeddings shapes:{[embedding.size() for embedding in sentence_embedding(english_batch, start_token = False, end_token = True)]}\\nOuput embeddings: {sentence_embedding(english_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence_embedding = SentenceEmbedding(max_sequence_len, d_model, english_to_index, nlp_en.tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "it_sentence_embedding = SentenceEmbedding(max_sequence_len, d_model, italian_to_index, nlp_it.tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_batch, italian_batch = next(iterator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/image2.png\" width = \"50%\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/attention.svg\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties:\n",
    "1. No access to \"future\" information $\\rightarrow$ autoregressive generation\n",
    "2. Each computation is independent $\\rightarrow$ parallelization\n",
    "\n",
    "Steps:\n",
    "1. Comparison\n",
    "$$\n",
    "\\text{score}(\\mathbf{x_i}, \\mathbf{x_j}) = \\mathbf{x_i} \\cdot \\mathbf{x_j}, \\quad \\forall j \\leq i\n",
    "$$\n",
    "2. Normalization\n",
    "$$\n",
    "\\alpha_{ij} = \\text{softmax}(\\text{score}(\\mathbf{x}_i, \\mathbf{x}_j)) = \\frac{exp(\\text{score}(\\mathbf{x}_i, \\mathbf{x}_j))}{\\sum_{k=1}^{i}exp(\\text{score}(\\mathbf{x}_i, \\mathbf{x}_k))}, \\quad \\forall j \\leq i\n",
    "$$\n",
    "3. Weighted sum\n",
    "$$\n",
    "\\mathbf{y}_i = \\sum_{j\\leq i}\\alpha_{ij}\\mathbf{x}_j\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention in transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different roles in attention process:\n",
    "1. current focus of attention $\\rightarrow$ **query**\n",
    "2. input being compared $\\rightarrow$ **key**\n",
    "3. output for current focus $\\rightarrow$ **value**\n",
    "<center><img src=\"img/query-key-value.svg\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "$$\n",
    "\\mathbf{Q} = \\mathbf{XW}^Q, \\quad \\mathbf{K} = \\mathbf{XW}^K, \\quad \\mathbf{V} = \\mathbf{XW}^V; \\quad \\mathbf{Q}, \\mathbf{K} \\in \\mathbb{R}^{N \\times d_k}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d_v}\n",
    "$$\n",
    "with $N$ being the number of input tokens and $d_k = d_v = d_{model}$ the dimensionality of input embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three steps are the same, but with matrices (parallel computation):\n",
    "1. Comparison\n",
    "$$\n",
    "\\text{score}(\\mathbf{Q}, \\mathbf{K}) = \\mathbf{Q}\\mathbf{K}^\\top\n",
    "$$\n",
    "2. Normalization (& scaling for stabilization)\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q},\\mathbf{K}) =\\text{softmax}\\left(\\frac{\\mathbf{QK^\\top}}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "3. Weighted sum\n",
    "$$\n",
    "\\text{Output}= \\text{softmax}\\left(\\frac{\\mathbf{QK^\\top}}{\\sqrt{d_k}}\\right)\\mathbf{V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k, d_v =  d_model, d_model\n",
    "w_q = torch.randn(d_k, d_k)\n",
    "w_k = torch.randn(d_k, d_k)\n",
    "w_v = torch.randn(d_v, d_v)\n",
    "input = en_sentence_embedding(english_batch, start_token = False, end_token = True)[0] #embeddings\n",
    "tokenized_sentence = en_sentence_embedding.batch_tokenize(english_batch, start_token = False, end_token = True)[0] #textual representation of sentence, tokenized\n",
    "tokenized_sentence_pad = [index_to_english[t.item()] for t in tokenized_sentence]\n",
    "tokenized_sentence_nopad = [index_to_english[t.item()] for t in tokenized_sentence if t != english_to_index[PADDING_TOKEN]]\n",
    "print(f\"Query weight matrix shape:\\t{w_q.size()}\")\n",
    "print(f\"Key weight matrix shape:\\t{w_k.size()}\")\n",
    "print(f\"Value weight matrix shape:\\t{w_v.size()}\")\n",
    "print(f\"Input matrix shape:\\t\\t{input.size()}\")\n",
    "\n",
    "#computing Q, K and V\n",
    "q = torch.matmul(input, w_q)\n",
    "k = torch.matmul(input, w_k)\n",
    "v = torch.matmul(input, w_v)\n",
    "print(f\"Query matrix shape:\\t\\t{q.size()}\")\n",
    "print(f\"Key matrix shape:\\t\\t{k.size()}\")\n",
    "print(f\"Value matrix shape:\\t\\t{v.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img, q_img, k_img, v_img = input[:len(tokenized_sentence_nopad),:] ,q[:len(tokenized_sentence_nopad),:], k[:len(tokenized_sentence_nopad),:], v[:len(tokenized_sentence_nopad),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (torch.exp(x).T / torch.sum(torch.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = torch.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  attention = softmax(scaled)\n",
    "  output = torch.matmul(attention, v)\n",
    "  return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, att = scaled_dot_product_attention(q, k, v)\n",
    "print(f\"Output shape:\\t\\t{o.size()}\")\n",
    "print(f\"Attention shape:\\t{att.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, att = scaled_dot_product_attention(input_img, input_img, v_img)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(att.detach().numpy(), xticklabels=tokenized_sentence_nopad, yticklabels=tokenized_sentence_nopad, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick visualization\n",
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "sentence_a = \"The cat sat on the mat\"\n",
    "sentence_b = \"The cat lay on the rug\"\n",
    "model_type = 'bert'\n",
    "model_version = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "show(model, model_type, tokenizer, sentence_a, sentence_b, layer=4, head=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention in Transformers is used in three different ways:\n",
    "- as the encoder's multi-head *self-attention*\n",
    "- as the decoders' **masked** multi-head *self-attention*\n",
    "- as the decoder's multi-head *cross-attention*\n",
    "\n",
    "Differences reside in the *masked* inputs and in source for $\\mathbf{Q}, \\mathbf{K}$ and $\\mathbf{V} $. Let's see why and how."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Masking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By 'masking' we mean setting a value to $-\\infty$. This leads to a 0-value output after the attention's softmax computation, which is exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to mask -> infinitely small number: needed when it will be passed \n",
    "#through softmax and to avoid 0/0 errors\n",
    "NEG_INFTY = -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_batch[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder's multi-head self attention and decoder's multi-head cross attention does not need any mask. Indeed we want it to extract both right context and left context. The only mask we will apply will block attention to and from \\<PADDING\\> tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_padding_mask(eng_batch):\n",
    "    masks = []\n",
    "    tokenized_batch = en_sentence_embedding.batch_tokenize(eng_batch, start_token = False, end_token = True)\n",
    "    for sentence in tokenized_batch:\n",
    "        row = torch.where(sentence == en_sentence_embedding.language_to_index[en_sentence_embedding.PADDING_TOKEN],\n",
    "                        True, False)\n",
    "        mask = torch.tile(row.unsqueeze(0), (len(sentence), 1))\n",
    "        mask = torch.logical_or(mask, mask.T)\n",
    "        mask = torch.where(mask, NEG_INFTY, 0)\n",
    "        masks.append(mask)        \n",
    "    masks = torch.stack(masks)\n",
    "    return masks.to(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder_padding_mask(eng_batch, it_batch):\n",
    "    masks = []\n",
    "    tokenized_it_batch = it_sentence_embedding.batch_tokenize(it_batch, start_token = True, end_token = True)\n",
    "    tokenized_en_batch = en_sentence_embedding.batch_tokenize(eng_batch, start_token = False, end_token = True)\n",
    "    masks = []\n",
    "    for en_sentence, it_sentence in zip(tokenized_en_batch, tokenized_it_batch):\n",
    "        en_row = torch.where(en_sentence == en_sentence_embedding.language_to_index[en_sentence_embedding.PADDING_TOKEN],\n",
    "                        True, False)\n",
    "        en_mask = torch.tile(en_row.unsqueeze(0), (len(en_sentence), 1))\n",
    "        it_row = torch.where(it_sentence == it_sentence_embedding.language_to_index[it_sentence_embedding.PADDING_TOKEN],\n",
    "                        True, False)\n",
    "        it_mask = torch.tile(it_row.unsqueeze(0), (len(it_sentence), 1))\n",
    "        mask = torch.logical_or(it_mask, en_mask.T)\n",
    "        mask = torch.where(mask, NEG_INFTY, 0)\n",
    "        masks.append(mask)        \n",
    "    masks = torch.stack(masks)  \n",
    "    return masks.to(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = create_encoder_padding_mask(english_batch)[0]\n",
    "fig, ax = plt.subplots(figsize=(13,10))  \n",
    "sns.heatmap(m, xticklabels=tokenized_sentence_pad, yticklabels=tokenized_sentence_pad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder's cross attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence_it = it_sentence_embedding.batch_tokenize(italian_batch, start_token = True, end_token = True)[0]\n",
    "tokenized_sentence_it_pad = [index_to_italian[t.item()] for t in tokenized_sentence_it]\n",
    "m = create_decoder_padding_mask(english_batch, italian_batch)[0]\n",
    "fig, ax = plt.subplots(figsize=(13,10))  \n",
    "sns.heatmap(m, xticklabels=tokenized_sentence_it_pad, yticklabels=tokenized_sentence_pad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder's masked multi head self attention needs look-ahead masks because it needs to learn to predict the next token: not masking future inputs would be cheating. More formally, remember we need to retain the auto-regressive property of attention to be able to generate text.\\\n",
    "For the decoder's self attention we also need \\<PADDING\\> token mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder_masked_attention_mask(it_batch):\n",
    "    tokenized_batch = it_sentence_embedding.batch_tokenize(it_batch, start_token = True, end_token = True)\n",
    "    \n",
    "    masks = []\n",
    "    for sentence in tokenized_batch:\n",
    "        look_ahead_mask = torch.full([len(sentence), len(sentence)] , True)\n",
    "        look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "        row = torch.where(sentence == it_sentence_embedding.language_to_index[it_sentence_embedding.PADDING_TOKEN],\n",
    "                        True, False)\n",
    "        padding_mask = torch.tile(row.unsqueeze(0), (len(sentence), 1))\n",
    "        padding_mask = torch.logical_or(padding_mask, padding_mask.T)\n",
    "        mask = torch.logical_or(padding_mask, look_ahead_mask)\n",
    "        mask = torch.where(mask, NEG_INFTY, 0)\n",
    "        masks.append(mask)        \n",
    "    masks = torch.stack(masks)\n",
    "    \n",
    "    return masks.to(get_device())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = create_decoder_masked_attention_mask(italian_batch)[0]\n",
    "fig, ax = plt.subplots(figsize=(13,10))  \n",
    "sns.heatmap(m, xticklabels=tokenized_sentence_it_pad, yticklabels=tokenized_sentence_it_pad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Attention Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine the attention function so that it accepts masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention, v)\n",
    "  return out, attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\\\n",
    "They give enriched contextual representation: how do different words relate to each other simultaneously?\\\n",
    "These are sets of self-attention layers, called **heads**, that reside in parallel layers at the same depth in a model, each with its own set of parameters. Given these distinct set of parameters, each head can learn different aspects of the relationships that exist among inputs at the same level of abstraction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each head takes a split of the original $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ matrices. In the original paper, the authors employ $h = 8$ parallel attention layers, called **heads**, where each head has a reduced input embedding dimensionality $d_{model}/h = 64$, being $d_{model}=512$.\\\n",
    "For the $i$-th attention head, \n",
    "$$\n",
    "\\mathbf{Q} = \\mathbf{XW}_i^Q, \\mathbf{K} = \\mathbf{XW}_i^K, \\mathbf{V} = \\mathbf{XW}_i^V; \\quad \\mathbf{W}_i^Q \\in \\mathbb{R}^{d_{model}\\times d_k}, \\mathbf{W}_i^K \\in \\mathbb{R}^{d_{model}\\times d_k}, \\mathbf{W}_i^V \\in \\mathbb{R}^{d_{model}\\times d_v} \n",
    "$$\n",
    "$$\n",
    "\\mathbf{head}_i = \\text{SelfAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})\n",
    "$$\n",
    "Then, we simply concatenate all $h$ outputs:\n",
    "$$\n",
    "\\text{MultiHeadAttn}(\\mathbf{X}) = (\\mathbf{head}_1 \\oplus \\mathbf{head}_2, \\dots, \\oplus  \\mathbf{head}_h) \\mathbf{W}^O, \\quad \\mathbf{W}^O \\in \\mathbb{R}^{hd_{v}\\times d_{model}} \n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say $h=4$: then\n",
    "<center><img src=\"img/multihead-attn.svg\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = en_sentence_embedding(english_batch, start_token = False, end_token = True)\n",
    "sequence_length = max_sequence_len\n",
    "print(f\"X shape: {input.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will stack our Q, K and V matrices in one single linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "qkv = qkv_layer(input)\n",
    "print(f\"QKV stacked matrix shape: {qkv.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim) #split the last dimension in two, [num_heads and 3 * head_dim]\n",
    "print(f\"QKV stacked matrix shape after split data for {num_heads} heads: {qkv.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this shape is equal to $\\texttt{[batch\\_size, sequence\\_length, num\\_heads, 3*head\\_dim]}$.\\\n",
    "We want it to be $\\texttt{[batch\\_size, num\\_heads, sequence\\_length, 3*head\\_dim]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "print(f\"QKV stacked matrix shape after split data for {num_heads} heads, dimensions permuted: {qkv.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only have to (actually) split this matrix in 3 sub-matrices to compute attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1) #breaking down last dimension\n",
    "print(f\"Q matrix from QKV stacked matrix: {q.size()}\")\n",
    "print(f\"K matrix from QKV stacked matrix: {k.size()}\")\n",
    "print(f\"V matrix from QKV stacked matrix: {v.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compute attention. First, we must determine $d_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = d_model // num_heads #equal to last dimension of q\n",
    "d_k = q.size()[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the matrix multiplication $\\mathbf{QK}^\\top$ must be coded such that only the last two dimensions of a vector with shape  $\\texttt{[batch\\_size, num\\_heads, sequence\\_length, 3*head\\_dim]}$ must be transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "print(f\"Scaled QK matrix: {scaled.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute and apply the mask for this encoder self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = create_encoder_padding_mask(english_batch)\n",
    "mask = torch.tile(mask.unsqueeze(1), (1, num_heads, 1, 1)) #add a dummy dimension to replicate mask for each split\n",
    "scaled = scaled + mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the masked scaled input, let's normalize the values through softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = F.softmax(scaled, dim=-1) #computed only on last dimension containing scaled input\n",
    "print(f\"Attention weights matrix: {attention_weights.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the output of each head in multihead attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_output = torch.matmul(attention_weights, v)\n",
    "print(f\"Output matrix for all heads: {heads_output.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output will simply be the reshaping of all heads outputs, averaged through a linear layer ($\\mathbf{W}^O$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_output = heads_output.reshape(input.size())\n",
    "print(f\"Output matrix for all heads after reshaping, ready for averaging: {heads_output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(d_model, d_model)\n",
    "output = linear_layer(heads_output)\n",
    "print(f\"Final output matrix after averaging: {output.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fancy class for Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiheadAttention(d_model, num_heads)\n",
    "out = model(input)\n",
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplications can easily make matrix values exploding or vanishing.\\\n",
    " Layer normalization helps with this possible problem, keeping the values in a range that facilitates gradient-based training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a hidden layer with dimensionality $d_h$, these values are calculated as follows\n",
    "$$\n",
    "\\mu = \\frac{1}{d_h}\\sum_{i = 1}^{d_h}x_i\n",
    "$$\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{d_h}\\sum_{i = 1}^{d_h}(x_i - \\mu)^2}\n",
    "$$\n",
    "and the normalized values will be computed as \n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\frac{\\mathbf{x}-\\mu}{\\sigma}\n",
    "$$\n",
    "Finally, in the standard implementation of layer normalization, two learnable parameters $\\gamma$ and $\\beta$, representing gain and offset values, are introduced\n",
    "$$\n",
    "\\text{LayerNorm} = \\gamma \\hat{\\mathbf{x}}+\\beta\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the mean as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we average on the last two dimensions\n",
    "mean = out.mean(dim = (-1, -2), keepdim=True) #keepdim=True retrieves a vector maintaining the number of dimensions of input\n",
    "print(mean.size())\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for standard deviation\n",
    "std = out.std(dim = (-1, -2), keepdim=True) #keepdim=True retrieves a vector maintaining the number of dimensions of input\n",
    "print(std.size())\n",
    "print(std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the normalized output is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_shape = out.size()[-2:]\n",
    "print(parameter_shape)\n",
    "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "beta =  nn.Parameter(torch.zeros(parameter_shape))\n",
    "\n",
    "out_norm = (out - mean)/std\n",
    "out_norm = gamma * out_norm + beta\n",
    "print(out_norm.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Old mean and variance:\\n{out.mean(dim = (-1, -2), keepdim=True)}\\n{out.var(dim = (-1, -2), keepdim=True)}\")\n",
    "print()\n",
    "print(f\"New mean and variance:\\n{out_norm.mean(dim = (-1, -2), keepdim=True)}\\n{out_norm.var(dim = (-1, -2), keepdim=True)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fancy class for Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, input):\n",
    "        dims = (-1, -2)\n",
    "        mean = input.mean(dim=dims, keepdim=True)\n",
    "        var = ((input - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt() #adding espilon to avoid 0 values\n",
    "        y = (input - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNormalization(out.size()[-2:])\n",
    "output_norm = layer_norm(out)\n",
    "output_norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual connections are connections that pass information from a lower layer to a higher layer without going through the intermediate layer.\\\n",
    "They are essential to strengthen the signals in deep neural networks as gradients could vanish when performing a high number of matrix multiplications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled.permute(1, 0, 2, 3) + mask #swap batch and head dimensions to match dimensions with mask\n",
    "        scaled = scaled.permute(1, 0, 2, 3) #resetting\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, input):\n",
    "        dims = (-1, -2)\n",
    "        mean = input.mean(dim=dims, keepdim=True)\n",
    "        var = ((input - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt() #adding espilon to avoid 0 values\n",
    "        y = (input - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out\n",
    "\n",
    "   \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        # make network understand better complex pattern\n",
    "        x = self.relu(x)\n",
    "        #better generalization\n",
    "        x = self.dropout(x)\n",
    "        #compress to 512 dimension\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "  \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        residual_x = x\n",
    "        x = self.attention(x, mask= self_attention_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x\n",
    "\n",
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, self_attention_mask  = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        #more layers for better vector representation for words and context\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_sequence_length = 200\n",
    "ffn_hidden = 2048 #as in paper, helps with propagation of information\n",
    "num_layers = 5 #multi to capture complexity\n",
    "\n",
    "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, english_to_index, nlp_en.tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "english_batch = next(iterator)[0]\n",
    "input = en_sentence_embedding(english_batch, start_token = False, end_token = True)\n",
    "out_encoder = encoder(input, create_encoder_padding_mask(english_batch))\n",
    "out_encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
    "        self.q_layer = nn.Linear(d_model , d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, y, mask):\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "        kv = self.kv_layer(x)\n",
    "        q = self.q_layer(y)\n",
    "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
    "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.reshape(batch_size, sequence_length, d_model)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
    "        _y = y.clone()\n",
    "        y = self.self_attention(y, mask=self_attention_mask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.layer_norm1(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.layer_norm2(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.ffn(y)\n",
    "        y = self.layer_norm3(y + _y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
    "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_sequence_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5\n",
    "\n",
    "it_sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, italian_to_index, nlp_it.tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "italian_batch = next(iterator)[1]\n",
    "input_it = it_sentence_embedding(italian_batch, start_token = True, end_token = True)\n",
    "self_attention_decoder_mask = create_decoder_masked_attention_mask(italian_batch)\n",
    "cross_attention_mask = create_decoder_padding_mask(english_batch, italian_batch)\n",
    "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "out_decoder = decoder(out_encoder, input_it, self_attention_decoder_mask, cross_attention_mask)\n",
    "out_decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedding(nn.Module):\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, lang_tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.language_tokenizer = lang_tokenizer\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token, end_token):\n",
    "\n",
    "        def tokenize(sentence, start_token, end_token):\n",
    "            try:\n",
    "                sentence_word_ids = [self.language_to_index[token.text] for token in self.language_tokenizer(sentence)]\n",
    "            except KeyError:\n",
    "                print(f'Invalid input token: token unknown')\n",
    "                raise KeyError\n",
    "            if start_token:\n",
    "                sentence_word_ids.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_ids.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_ids), self.max_sequence_length):\n",
    "                sentence_word_ids.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            \n",
    "            return torch.tensor(sentence_word_ids)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "           tokenized.append(tokenize(batch[sentence_num], start_token, end_token))\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(get_device())\n",
    "    \n",
    "    def forward(self, x, start_token, end_token):\n",
    "        x = self.batch_tokenize(x, start_token, end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 language_to_index,\n",
    "                 language_tokenizer,\n",
    "                 START_TOKEN,\n",
    "                 END_TOKEN, \n",
    "                 PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, language_tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        self_attention_mask = create_encoder_padding_mask(x)\n",
    "        x = self.sentence_embedding(x, start_token = False, end_token = True)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 language_to_index, \n",
    "                 language_tokenizer,\n",
    "                 START_TOKEN,\n",
    "                 END_TOKEN, \n",
    "                 PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, language_tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, y, decoder_self_attention_mask, decoder_cross_attention_mask):\n",
    "        y = self.sentence_embedding(y, start_token = True, end_token = True)\n",
    "        y = self.layers(x, y, decoder_self_attention_mask, decoder_cross_attention_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                ffn_hidden, \n",
    "                num_heads, \n",
    "                drop_prob, \n",
    "                num_layers,\n",
    "                max_sequence_length, \n",
    "                english_to_index,\n",
    "                english_tokenizer,\n",
    "                italian_to_index,\n",
    "                italian_tokenizer,\n",
    "                index_to_italian,\n",
    "                START_TOKEN, \n",
    "                END_TOKEN, \n",
    "                PADDING_TOKEN,\n",
    "                logging = False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, english_tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, italian_to_index, italian_tokenizer, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.linear = nn.Linear(d_model, len(italian_to_index))\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.index_to_italian = index_to_italian\n",
    "        if logging:\n",
    "            print('TRANSFORMER - architecture')\n",
    "            print(f'Internal embeddings dimension (d_model): {d_model}')\n",
    "            print(f'Hidden feedforward layer dimension (ffn_hidden): {ffn_hidden}')\n",
    "            print(f'Number of attention heads in Multi-head attention (num_heads): {num_heads}')\n",
    "            print(f'Dropout probability in Dropout layer (drop_prob): {drop_prob}')\n",
    "            print(f'Number of Encoder/Decoder layers (num_layers): {num_layers}')\n",
    "            print(f'Maximum numbers of tokens in an input sequence (max_sequence_length): {max_sequence_length}')\n",
    "\n",
    "    def forward(self, \n",
    "                x, \n",
    "                y):\n",
    "        encoder_self_attention_mask = create_encoder_padding_mask(x)\n",
    "        decoder_self_attention_mask = create_decoder_masked_attention_mask(y)\n",
    "        decoder_cross_attention_mask = create_decoder_padding_mask(x, y)\n",
    "        x = self.encoder(x, encoder_self_attention_mask)\n",
    "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask)\n",
    "        out = self.linear(out)\n",
    "        out = self.softmax(out)\n",
    "        out = torch.argmax(out, dim = -1, keepdim=True).squeeze(-1)\n",
    "        out_sentences = []\n",
    "        for sentence in out:\n",
    "                out_sentences.append(' '.join([self.index_to_italian[idx.item()] for idx in sentence]))\n",
    "        return out, out_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "ffn_hidden = 2048\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 6\n",
    "max_sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = Transformer(d_model,\n",
    "                                ffn_hidden,\n",
    "                                num_heads, \n",
    "                                drop_prob,\n",
    "                                num_layers,\n",
    "                                max_sequence_length,\n",
    "                                english_to_index,\n",
    "                                nlp_en.tokenizer,\n",
    "                                italian_to_index,\n",
    "                                nlp_it.tokenizer,\n",
    "                                index_to_italian,\n",
    "                                START_TOKEN,\n",
    "                                END_TOKEN,\n",
    "                                PADDING_TOKEN,\n",
    "                                logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = TextDataset(['my favourite animal is the raccoon.',\n",
    "                        'yesterday i saw a raccoon.'], \n",
    "                        ['',\n",
    "                        ''])\n",
    "batch_size_1 = 2\n",
    "train_loader_1 = DataLoader(dataset_1, batch_size_1)\n",
    "iterator_1 = iter(train_loader_1)\n",
    "input_batch_1 = next(iterator_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer_model(*input_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory & Intuition\n",
    "- **[Vaswani et al., 2017]** Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” In Advances in Neural Information Processing Systems, Vol. 30. Curran Associates, Inc., 2017. https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n",
    "- *\"Speech and Language Processing\"* (2022), Dan Jurafsky and James H.Martin, Chapters 9, 10, 11\n",
    "- *\"Illustrated Guide to Transformers Neural Network: A step by step explanation\"*: https://youtu.be/4Bdc55j80l8\n",
    "- *\"Transformers, explained: Understand the model behind GPT, BERT, and T5\"*: https://youtu.be/SZorAJ4I-sA\n",
    "- *\"Transformer Neural Networks - EXPLAINED! (Attention is all you need)\"*: https://youtu.be/TQQlZhbC5ps\n",
    "- *\"The complete guide to Transformer neural Networks!\"*: https://youtu.be/Nw_PJdmydZY\n",
    "\n",
    "Code\n",
    "- *\"Transformers from scratch\"* playlist: https://youtube.com/playlist?list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
