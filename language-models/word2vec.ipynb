{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Gensim implementation\n",
    "> Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., & Joulin, A. (2017). Advances in pre-training distributed word representations. arXiv preprint arXiv:1712.09405."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format(datapath(\"/Users/flint/Data/word2vec/GoogleNews-vectors-negative300.bin\"), \n",
    "                                       binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle 0.7821096181869507\n",
      "cars 0.7423831224441528\n",
      "SUV 0.7160962224006653\n",
      "minivan 0.6907036900520325\n",
      "truck 0.6735789775848389\n",
      "Car 0.6677608489990234\n",
      "Ford_Focus 0.667320191860199\n",
      "Honda_Civic 0.6626849174499512\n",
      "Jeep 0.651133120059967\n",
      "pickup_truck 0.6441438794136047\n"
     ]
    }
   ],
   "source": [
    "for x, y in wv.most_similar('car'):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for word in ['car', 'minivan', 'bicycle', 'airplane']:\n",
    "    vectors.append(wv.get_vector(word))\n",
    "V = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = V.mean(axis=0)\n",
    "v = v - wv.get_vector('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LightHawk', 0.3567371666431427),\n",
       " ('Beaver_floatplane', 0.3410896956920624),\n",
       " ('Bluebills', 0.3352811932563782),\n",
       " ('airplane', 0.32490819692611694),\n",
       " ('Volk_Field', 0.307051420211792),\n",
       " ('Andersland', 0.30294421315193176),\n",
       " ('Expedia_Expedia.com', 0.30243098735809326),\n",
       " ('NASA_Weightless_Wonder', 0.30234652757644653),\n",
       " ('Cessna_###B', 0.2979174852371216),\n",
       " ('propeller_plane', 0.2970975339412689)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy\n",
    "\n",
    "FRANCE : PARIS = ITALY : ?\n",
    "\n",
    "PARIS - FRANCE + ITALY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Milan', 0.7222141623497009),\n",
       " ('Rome', 0.702830970287323),\n",
       " ('Palermo_Sicily', 0.5967570543289185),\n",
       " ('Italian', 0.5911272764205933),\n",
       " ('Tuscany', 0.5632812976837158),\n",
       " ('Bologna', 0.5608358383178711),\n",
       " ('Sicily', 0.5596384406089783),\n",
       " ('Bologna_Italy', 0.5470058917999268),\n",
       " ('Berna_Milan', 0.5464027523994446),\n",
       " ('Genoa', 0.5308900475502014)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['Paris', 'Italy'], negative=['France'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match(\"school professor apple student\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = wv['school']\n",
    "vr = wv['professor']\n",
    "vx = wv['student']\n",
    "m = (vp + vr + vx) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 0.8481254577636719),\n",
       " ('professor', 0.7627506852149963),\n",
       " ('teacher', 0.6942789554595947),\n",
       " ('school', 0.6849855780601501),\n",
       " ('students', 0.6768636703491211),\n",
       " ('lecturer', 0.6700003147125244),\n",
       " ('faculty', 0.645453155040741),\n",
       " ('university', 0.6376535892486572),\n",
       " ('professors', 0.6346085667610168),\n",
       " ('associate_professor', 0.6325882077217102)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lecturer'\t'school'\t0.18\n",
      "'lecturer'\t'professor'\t0.80\n",
      "'lecturer'\t'student'\t0.43\n",
      "'lecturer'\t'teacher'\t0.48\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('lecturer', 'school'),\n",
    "    ('lecturer', 'professor'),\n",
    "    ('lecturer', 'student'),\n",
    "    ('lecturer', 'teacher'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sell', 0.8308461308479309),\n",
       " ('purchase', 0.7639904618263245),\n",
       " ('buying', 0.7209187746047974),\n",
       " ('bought', 0.7087081074714661),\n",
       " ('buys', 0.6617438197135925),\n",
       " ('Buy', 0.5850198864936829),\n",
       " ('tobuy', 0.5843992829322815),\n",
       " ('purchased', 0.582695484161377),\n",
       " ('Buying', 0.578020453453064),\n",
       " ('acquire', 0.5730165839195251)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31760776"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('buy', 'money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a global model for YELP reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_file = '../lexicon/data/yelp_sample.json'\n",
    "with open(review_data_file, 'r') as infile:\n",
    "    R = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"Red, white and bleu salad was super yum and a great addition to the menu! This location was clean with great service and food served at just the right temps! Kids pizza is always a hit too with lots of great side dish options for the kiddos! When I'm on this side of town, this will definitely be a spot I'll hit up again!\",\n",
       " 'date': '2014-02-17',\n",
       " 'stars': 4,\n",
       " 'useful': 1,\n",
       " 'funny': 0,\n",
       " 'cool': 0,\n",
       " 'business': 'Ue6-WhXvI-_1xUIuapl0zQ',\n",
       " 'id': '----X0BIDP9tA49U3RvdSQ',\n",
       " 'categories': ['American (Traditional)', 'Burgers', 'Restaurants']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[x.lower() for x in word_tokenize(doc['content']) if x not in punctuation] for doc in R]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'white', 'and', 'bleu', 'salad', 'was']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "R0 = gensim.models.Word2Vec(sentences=data, epochs=25, window=6, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vehicle', 0.7438929080963135),\n",
       " ('battery', 0.6146064400672913),\n",
       " ('bike', 0.6076012849807739),\n",
       " ('dealership', 0.5746821165084839),\n",
       " ('suit', 0.5642380118370056),\n",
       " ('teeth', 0.5638452172279358),\n",
       " ('contract', 0.5471097826957703),\n",
       " ('bank', 0.5426754951477051),\n",
       " ('tire', 0.5422413349151611),\n",
       " ('property', 0.5409041047096252)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R0.wv.most_similar('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application example: use graph community detection to find aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68efbc514ffe48c899ce52dfd161e30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_sim = 0.7\n",
    "G = nx.Graph()\n",
    "for word in tqdm(R0.wv.index_to_key):\n",
    "    for match, sim in R0.wv.most_similar(word):\n",
    "        if sim >= min_sim:\n",
    "            G.add_edge(word, match, sim=sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you u {'sim': 0.778407633304596}\n"
     ]
    }
   ],
   "source": [
    "for a, b, c in G.edges(data=True):\n",
    "    print(a, b, c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = Network('1500px', '1500px')\n",
    "nt.from_nx(G.subgraph(list(G.nodes)[:100]))\n",
    "nt.show('word2vec.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chocolate', 'spiced', 'salami', 'masala', 'mahi', 'flatbread', 'seed', 'risotto', 'skins', 'peppers']\n",
      "[\"j'ai\", 'belle', 'sans', 'mes', 'suis', 'pu', 'même', 'vous', 'et', 'cette']\n",
      "['der', 'alles', 'hatte', 'nur', 'ganz', 'empfehlung', 'es', 'waren', 'über', 'das']\n",
      "['cheesy', 'tender', 'flavour', 'buttery', 'hints', 'thick', 'juicy', 'doughy', 'sugary', 'sticky']\n",
      "['yu', 'attentive', 'polite', 'talented', 'dom', 'demeanor', 'skilled', 'respectful', 'gentle', 'professional']\n",
      "['hero', 'dumplings', 'bone-in', 'kabob', 'chops', 'kung', 'pao', 'siu', 'szechuan', 'corned']\n",
      "['sexy', 'decorated', 'spacious', 'atmosphere', 'interior', 'quiet', 'chic', 'upbeat', 'elegant', 'cozy']\n",
      "['screwed', 'messed', 'grown', 'hyped', 'wrapping', 'sums', 'popping', 'hung', 'summed', 'mt']\n",
      "['sister', 'husband', 'coworker', 'brother', 'son', 'hubby', 'wife', 'father', 'friend', 'boyfriend']\n",
      "['forty', '15-20', '50', 'fifteen', '90', '20', '35', 'thirty', '40', '45']\n",
      "['unassuming', 'ultrasound', 'institution', 'assortment', 'understatement', 'emergency', 'avid', 'ad', 'annual', 'embarrassment']\n",
      "['7pm', '9:30', 'pm', '3pm', '1pm', '8:30pm', 'midnight', '4pm', '9pm', '2pm']\n",
      "['5', 'three', '2.5', 'six', 'five', '0', 'two', '2', 'four', '9']\n",
      "['downtown', 'south', 'phx', 'pittsburgh', 'east', 'north', 'toronto', 'henderson', 'arizona', 'valley']\n",
      "['awesome', 'fabulous', 'fantastic', 'superb', 'decent', 'outstanding', 'incredible', 'wonderful', 'phenomenal', 'great']\n",
      "['mushy', 'tasteless', 'salty', 'flavorless', 'bland', 'soggy', 'overcooked', 'burnt', 'oily', 'undercooked']\n",
      "['bigger', 'nicer', 'thicker', 'more', 'less', 'sweeter', 'smaller', 'pricier', 'hotter', 'cheaper']\n",
      "['acrylic', 'gel', 'lashes', 'manicure', 'eyebrows', 'shellac', 'pedicure', 'hair', 'pedi', 'nails']\n",
      "['planet', 'bellagio', 'carlo', 'monte', 'venetian', 'hollywood', 'mgm', 'tower', 'cosmopolitan', 'mirage']\n",
      "['april', '2015', 'february', 'october', '2016', 'november', 'july', 'march', 'december', 'september']\n",
      "['handful', 'couple', 'ton', 'tons', 'several', 'few', 'numerous', 'lots', 'bunch', 'plenty']\n",
      "['month', 'hours', 'visits', 'years', 'weeks', 'yrs', 'year', 'days', 'attempts', 'months']\n",
      "['sinks', 'shower', 'tub', 'toilet', 'bathtub', 'kitchenette', 'bathroom', 'bedroom', 'refrigerator', 'jacuzzi']\n",
      "['monday', 'weekday', 'friday', 'wednesday', 'thursday', 'tuesday', 'sunday', 'saturday']\n",
      "['fusion', 'vietnamese', 'filipino', 'chinese', 'korean', 'asian', 'mexican', 'indian']\n",
      "['scrambled', 'yolk', 'florentine', 'deviled', 'gratin', 'poached', 'benedict', 'hollandaise']\n",
      "['adovada', 'asada', 'carne', 'asado', 'barbacoa', 'tamales', 'carnitas', 'empanadas']\n",
      "['may', 'will', \"'ll\", 'should', 'would', \"'d\", 'might']\n",
      "['meats', 'vegetables', 'flavours', 'toppings', 'flavors', 'fruits', 'sauces']\n",
      "['favorites', 'fave', 'favourite', 'go-to', 'favorite', 'fav']\n",
      "['restaurants', 'spots', 'establishments', 'hotels', 'places']\n",
      "['iced', 'bubble', 'milk', 'chai', 'teas']\n",
      "['satisfied', 'unhappy', 'pleased', 'impressed', 'disappointed']\n",
      "['min', 'mins', 'minute', 'seconds', 'minutes']\n",
      "['kinds', 'varieties', 'pasties', 'sorts', 'types']\n",
      "['chow', 'mein', 'lo', 'stew', 'wonton']\n",
      "['inquired', 'rave', 'raving', 'raved', 'raves']\n",
      "['super', 'extremely', 'very', 'incredibly']\n",
      "['handed', 'brought', 'came', 'went']\n",
      "['reccomend', 'recommend', 'suggest', 'reccommend']\n",
      "['told', 'assured', 'informed', 'advised']\n",
      "['server', 'waiter', 'bartender', 'waitress']\n",
      "['lady', 'gentleman', 'woman', 'girl']\n",
      "['burgers', 'salads', 'soups', 'sandwiches']\n",
      "['waitresses', 'bartenders', 'waiters', 'servers']\n",
      "['pastries', 'lattes', 'coffees', 'gelato']\n",
      "['restaurant', 'establishment', 'place']\n",
      "['worst', 'best', 'rudest']\n",
      "['ordered', 'shared', 'got']\n",
      "['night', 'afternoon', 'evening']\n",
      "['asked', 'ask', 'asking']\n",
      "['dinner', 'lunch', 'brunch']\n",
      "['size', 'portion', 'portions']\n",
      "['unfriendly', 'rude', 'unprofessional']\n",
      "['affordable', 'reasonable', 'competitive']\n",
      "['deposit', 'credit', 'card']\n",
      "['terrible', 'awful', 'horrible']\n",
      "['perfectly', 'nicely', 'perfection']\n",
      "['located', 'plaza', 'mall']\n",
      "['dr.', 'dental', 'dr']\n",
      "['dollars', 'bucks', 'cents']\n",
      "['complaint', 'gripe', 'criticism']\n",
      "['careful', 'supposed', 'suppose']\n",
      "['warranty', 'claim', 'insurance']\n",
      "['areas', 'chairs', 'booths']\n",
      "['spas', 'eateries', 'locations']\n",
      "['games', 'televisions', 'tvs']\n",
      "['indoor', 'lounge', 'outdoor']\n",
      "['wan', 'gon', 'na']\n",
      "['sashimi', 'maki', 'nigiri']\n",
      "['mary', 'bloody', 'marys']\n",
      "['downfall', 'downside', 'drawback']\n",
      "['san', 'francisco', 'diego']\n",
      "['replies', 'replied', 'responded']\n",
      "['thankful', 'proud', 'grateful']\n",
      "['refilled', 'waters', 'refilling']\n",
      "['versed', 'maintained', 'represented']\n",
      "['executive', 'julian', 'serrano']\n",
      "['paths', 'studios', 'buildings']\n",
      "['4-5', '2-3', '1-2']\n",
      "['hygienist', 'karen', 'nicole']\n",
      "['ttc', 'finch', 'india']\n",
      "['you', 'u']\n",
      "['not', \"n't\"]\n",
      "['go', 'come']\n",
      "[\"'m\", 'am']\n",
      "['need', 'want']\n",
      "['anything', 'something']\n",
      "['ca', 'wo']\n",
      "['small', 'large']\n",
      "['needed', 'wanted']\n",
      "['vehicle', 'car']\n",
      "['star', 'stars']\n",
      "['trip', 'visit']\n",
      "['shop', 'store']\n",
      "['variety', 'selection']\n",
      "['loved', 'enjoyed']\n",
      "['crowded', 'busy']\n",
      "['shaved', 'cream']\n",
      "['hotel', 'casino']\n",
      "['pork', 'brisket']\n",
      "['seasoned', 'cooked']\n",
      "['ok', 'okay']\n",
      "['options', 'choices']\n",
      "['high', 'low']\n",
      "['imagine', 'tell']\n",
      "['entire', 'whole']\n",
      "['help', 'assist']\n",
      "['parties', 'tables']\n",
      "['tacos', 'taco']\n",
      "['party', 'group']\n",
      "['gorgeous', 'beautiful']\n",
      "['appt', 'appointment']\n",
      "['expensive', 'pricey']\n",
      "['employees', 'workers']\n",
      "['view', 'views']\n",
      "['piece', 'slice']\n",
      "['entrees', 'appetizers']\n",
      "['complaining', 'talking']\n",
      "['dined', 'stayed']\n",
      "['glass', 'bottle']\n",
      "['somewhere', 'anywhere']\n",
      "['issues', 'problems']\n",
      "['purchased', 'bought']\n",
      "['sent', 'send']\n",
      "['melts', 'mouth']\n",
      "['reservation', 'reservations']\n",
      "['craving', 'hankering']\n",
      "['sour', 'whipped']\n",
      "['miso', 'noodle']\n",
      "['fees', 'fee']\n",
      "['doctor', 'tech']\n",
      "['groupon', 'coupon']\n",
      "['spoke', 'talked']\n",
      "['pub', 'diner']\n",
      "['answer', 'answered']\n",
      "['feels', 'smells']\n",
      "['hanging', 'hang']\n",
      "['walls', 'stage']\n",
      "['reasonably', 'decently']\n",
      "['message', 'email']\n",
      "['orleans', 'york']\n",
      "['telling', 'kidding']\n",
      "['clubs', 'bars']\n",
      "['ny', 'chicago']\n",
      "['1st', '2nd']\n",
      "['hint', 'balance']\n",
      "['joe', 'trader']\n",
      "['awkward', 'uncomfortable']\n",
      "['cole', 'slaw']\n",
      "['reminded', 'reminds']\n",
      "['wifi', 'wi-fi']\n",
      "['upscale', 'intimate']\n",
      "['consists', 'includes']\n",
      "['unit', 'account']\n",
      "['manner', 'timely']\n",
      "['martini', 'vodka']\n",
      "['impression', 'impressions']\n",
      "['expertise', 'knowledge']\n",
      "['mirror', 'floors']\n",
      "['auto', 'finance']\n",
      "['ceiling', 'pipes']\n",
      "['confronted', 'complained']\n",
      "['cheesesteak', 'philly']\n",
      "['wasted', 'wasting']\n",
      "['omelet', 'omelette']\n",
      "['pointed', 'handing']\n",
      "['technician', 'receptionist']\n",
      "['michael', 'jackson']\n",
      "['lawrence', 'st.']\n",
      "['tucked', 'nestled']\n",
      "['mead', 'lake']\n",
      "['scoop', 'jar']\n",
      "['eaton', 'centre']\n",
      "['banh', 'mi']\n",
      "['cast', 'iron']\n",
      "['gordon', 'ramsay']\n",
      "['chandler', 'gilbert']\n",
      "['hoover', 'dam']\n",
      "['impeccable', 'horrendous']\n",
      "['blvd', 'rd']\n",
      "['jmc', 'g']\n",
      "['avenue', 'st']\n",
      "['delighted', 'spoiled']\n",
      "['bang', 'buck']\n",
      "['compassion', 'genuine']\n",
      "['tater', 'tots']\n",
      "['waxing', 'facials']\n",
      "['upside', 'watered']\n",
      "['ave', 'decatur']\n",
      "['bridge', 'w.']\n",
      "['creek', 'cave']\n",
      "['suites', 'towers']\n",
      "['spirit', 'airlines']\n",
      "['shampoo', 'conditioner']\n",
      "['deliciously', 'charred']\n",
      "['jr.', 'carl']\n",
      "['odds', 'maximum']\n",
      "['path', 'beaten']\n",
      "['floored', 'intrigued']\n",
      "['mitchell', 'graeter']\n",
      "['exposed', 'marble']\n",
      "['decadent', 'addictive']\n",
      "['fanatic', 'addicted']\n",
      "['papa', 'johns']\n",
      "['roller', 'ceilings']\n",
      "['bo', 'hue']\n",
      "['kong', 'hong']\n",
      "['scoops', 'pounds']\n",
      "['mule', 'moscow']\n",
      "['pinot', 'noir']\n",
      "['celebrity', 'michelin']\n",
      "['rancheros', 'huevos']\n",
      "['pf', 'chang']\n",
      "['cotta', 'panna']\n",
      "['shepard', \"po'boy\"]\n",
      "['foremost', 'timer']\n",
      "['episode', 'icon']\n",
      "['acura', 'infiniti']\n",
      "['e-gift', 'certificate']\n",
      "['benches', 'cardio']\n",
      "['depot', 'lowes']\n",
      "['fresher', 'stronger']\n",
      "['chevrolet', 'hendrick']\n",
      "['a.m.', '12pm']\n",
      "['myun', 'naeng']\n",
      "['tutti', 'matti']\n",
      "['allergic', 'shellfish']\n",
      "['sq', '1800']\n",
      "['martin', 'dean']\n",
      "['rockets', 'johnny']\n",
      "['identity', 'crisis']\n",
      "['catherine', 'rue']\n",
      "['krispy', 'kreme']\n",
      "['playground', 'slides']\n",
      "['limes', 'lemons']\n",
      "['applying', 'lotion']\n",
      "['accents', 'chandeliers']\n",
      "['pei', 'wei']\n"
     ]
    }
   ],
   "source": [
    "communities = greedy_modularity_communities(G)\n",
    "for community in communities:\n",
    "    print(list(community)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update an existing model\n",
    "Let's create a collection for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = defaultdict(list)\n",
    "for review in R:\n",
    "    content, categories = review['content'], review['categories']\n",
    "    tokens = [x.lower() for x in word_tokenize(content) if x not in punctuation]\n",
    "    for category in categories:\n",
    "        corpora[category].append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unbelievable',\n",
       " 'how',\n",
       " 'the',\n",
       " 'employees',\n",
       " 'that',\n",
       " 'work',\n",
       " 'in',\n",
       " 'the',\n",
       " 'front',\n",
       " 'part']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora['RV Rental'][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the global model with the local information (i.e., create a model for each category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6674248c7674af081ab445e54304fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_categories = ['Burgers', 'Indian', 'Italian', 'Seafood']\n",
    "models = {}\n",
    "runs = [(c, d) for c, d in corpora.items() if c in selected_categories]\n",
    "for category, data in tqdm(runs):\n",
    "    models[category] = copy.deepcopy(R0)\n",
    "    models[category].train(data, total_examples=R0.corpus_count, epochs=R0.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Burgers', 'Indian', 'Italian', 'Seafood']\n"
     ]
    }
   ],
   "source": [
    "print(list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers ['sushi', 'sangria', 'ambiance', 'breakfasts', 'meal']\n",
      "Indian ['sushi', 'meal', 'pizza', 'it', 'drinks']\n",
      "Italian ['sushi', 'consistently', 'breakfasts', 'meal', 'ambience']\n",
      "Seafood ['sushi', 'sangria', 'presentation', 'fare', 'foods']\n"
     ]
    }
   ],
   "source": [
    "word = 'food'\n",
    "for cat, m in models.items():\n",
    "    print(cat, [x for x, y in m.wv.most_similar(word, topn=20)][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers ['atmosphere', 'pricing', 'insanely', 'environment', 'pho']\n",
      "Indian ['it', 'drinks', 'attentive', 'meals', 'restaurant']\n",
      "Italian ['atmosphere', 'margaritas', 'value', 'drinks', 'pricing', 'attentive', 'relatively', 'coffee', 'desserts', 'environment']\n",
      "Seafood ['foods', 'coffee', 'meals', 'schnitzel', 'sandwiches', 'insanely']\n"
     ]
    }
   ],
   "source": [
    "counter = defaultdict(lambda: 0)\n",
    "for cat, m in models.items():\n",
    "    for x, y in m.wv.most_similar(word, topn=20):\n",
    "        counter[x] += 1\n",
    "for cat, m in models.items():\n",
    "    print(cat, [x for x, y in m.wv.most_similar(word, topn=20) if counter[x] < 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: train a model from wordnet\n",
    "\n",
    "How `word2vec` may be used to disambiguate words and lookup for synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cat.n.01']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fish.v.01', 0.33891671895980835),\n",
       " ('shuttle', 0.3145788609981537),\n",
       " ('run_down', 0.29708608984947205),\n",
       " ('solid_food', 0.28112900257110596),\n",
       " ('brail', 0.27431318163871765),\n",
       " ('grownup', 0.27283111214637756),\n",
       " ('pictorial_representation', 0.209917813539505),\n",
       " ('eel.n.01', 0.20881067216396332),\n",
       " ('weenie', 0.2070414274930954),\n",
       " ('hot_dog', 0.19912771880626678)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "words = ['cat', 'dog', 'bird', 'fish']\n",
    "\n",
    "h = lambda s: s.hypernyms()\n",
    "p = lambda s: s.hyponyms()\n",
    "\n",
    "def get_pseudo_sentences(word, context=3):\n",
    "    sentences = []\n",
    "    for s in wn.synsets(word):\n",
    "        for lemma in s.lemmas():\n",
    "            sentences.append([lemma.name(), s.name()])\n",
    "        for i, j in enumerate(s.closure(h)):\n",
    "            sentences.append([s.name(), j.name()])\n",
    "            for lemma in j.lemmas():\n",
    "                sentences.append([lemma.name(), j.name()])\n",
    "            if i == context:\n",
    "                break\n",
    "        for i, j in enumerate(s.closure(p)):\n",
    "            sentences.append([j.name(), s.name()])\n",
    "            for lemma in j.lemmas():\n",
    "                sentences.append([lemma.name(), j.name()])\n",
    "            if i == context:\n",
    "                break\n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "for w in words:\n",
    "    sentences += get_pseudo_sentences(w)\n",
    "    \n",
    "print(sentences[0])\n",
    "\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.wv.most_similar('fish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
