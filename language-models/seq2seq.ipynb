{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dates(from_day, to_day):\n",
    "    return [pd.to_datetime(x).date() for x in pd.date_range(from_day, to_day, freq='D').values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dates = generate_dates('1960-01-01', '1961-12-31')\n",
    "pairs = []\n",
    "for d in initial_dates:\n",
    "    pair = [d.strftime('%d %B, %Y'), d.strftime('%d %b %Y')]\n",
    "    pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01 January, 1960', '01 Jan 1960']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tofile = \"\"\n",
    "for pair in pairs:\n",
    "    strf = \"{}\\t{}\\n\".format(pair[0], pair[1])\n",
    "    tofile += strf\n",
    "with open('./translate/date1-date3.txt', 'w') as out:\n",
    "    out.write(tofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
    "**Author**: [Sean Robertson](https://github.com/spro/practical-pytorch)\n",
    "\n",
    "This tutorial is almost identical to the [PyTorch version](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). We just substitute to sentences the date formats and we use characters sequences instead of word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"@\", 1: \"#\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('./translate/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 731 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "date1 40\n",
      "date3 35\n",
      "['12 November, 1961', '12 Nov 1961']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('date1', 'date3', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '@',\n",
       " 1: '#',\n",
       " 2: '0',\n",
       " 3: '1',\n",
       " 4: ' ',\n",
       " 5: 'J',\n",
       " 6: 'a',\n",
       " 7: 'n',\n",
       " 8: '9',\n",
       " 9: '6',\n",
       " 10: '2',\n",
       " 11: '3',\n",
       " 12: '4',\n",
       " 13: '5',\n",
       " 14: '7',\n",
       " 15: '8',\n",
       " 16: 'F',\n",
       " 17: 'e',\n",
       " 18: 'b',\n",
       " 19: 'M',\n",
       " 20: 'r',\n",
       " 21: 'A',\n",
       " 22: 'p',\n",
       " 23: 'y',\n",
       " 24: 'u',\n",
       " 25: 'l',\n",
       " 26: 'g',\n",
       " 27: 'S',\n",
       " 28: 'O',\n",
       " 29: 'c',\n",
       " 30: 't',\n",
       " 31: 'N',\n",
       " 32: 'o',\n",
       " 33: 'v',\n",
       " 34: 'D'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Decoder\n",
    "\n",
    "If only the context vector is passed between the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2 = tensorsFromPair(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11],\n",
       "        [ 4],\n",
       "        [ 3],\n",
       "        [12],\n",
       "        [13],\n",
       "        [ 2],\n",
       "        [ 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but [when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf)_.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ''.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 0m 22s) (200 10%) 2.2933\n",
      "0m 4s (- 0m 19s) (400 20%) 1.4039\n",
      "0m 7s (- 0m 17s) (600 30%) 0.8273\n",
      "0m 9s (- 0m 14s) (800 40%) 0.6562\n",
      "0m 12s (- 0m 12s) (1000 50%) 0.4561\n",
      "0m 14s (- 0m 9s) (1200 60%) 0.2402\n",
      "0m 17s (- 0m 7s) (1400 70%) 0.1126\n",
      "0m 19s (- 0m 4s) (1600 80%) 0.0646\n",
      "0m 22s (- 0m 2s) (1800 90%) 0.0387\n",
      "0m 25s (- 0m 0s) (2000 100%) 0.0251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 2000, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 22 May, 1961\n",
      "= 22 May 1961\n",
      "< 22 May 1961<EOS>\n",
      "\n",
      "> 24 August, 1960\n",
      "= 24 Aug 1960\n",
      "< 24 Aug 1960<EOS>\n",
      "\n",
      "> 14 February, 1960\n",
      "= 14 Feb 1960\n",
      "< 14 Feb 1960<EOS>\n",
      "\n",
      "> 04 December, 1961\n",
      "= 04 Dec 1961\n",
      "< 04 Nec 1961<EOS>\n",
      "\n",
      "> 24 November, 1960\n",
      "= 24 Nov 1960\n",
      "< 24 Nov 1960<EOS>\n",
      "\n",
      "> 16 April, 1961\n",
      "= 16 Apr 1961\n",
      "< 16 Apr 1961<EOS>\n",
      "\n",
      "> 01 May, 1960\n",
      "= 01 May 1960\n",
      "< 01 May 1960<EOS>\n",
      "\n",
      "> 21 October, 1960\n",
      "= 21 Oct 1960\n",
      "< 21 Oct 1960<EOS>\n",
      "\n",
      "> 22 April, 1960\n",
      "= 22 Apr 1960\n",
      "< 22 Apr 1960<EOS>\n",
      "\n",
      "> 27 October, 1960\n",
      "= 27 Oct 1960\n",
      "< 27 Oct 1960<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_date = \"19 January, 1961\"\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, input_date)\n",
    "ax.matshow(attentions.numpy(), cmap='Greens')\n",
    "ax.set_xticks(range(len(input_date) + 2))\n",
    "ax.set_xticklabels(['@'] + [x for x in input_date] + ['#'])\n",
    "ax.set_yticks(range(len(output_words)))\n",
    "ax.set_yticklabels([x for x in output_words])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
