{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Gensim implementation\n",
    "> Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., & Joulin, A. (2017). Advances in pre-training distributed word representations. arXiv preprint arXiv:1712.09405."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format(datapath(\"/Users/flint/Data/word2vec/GoogleNews-vectors-negative300.bin\"), \n",
    "                                       binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gorgeous 0.8353005051612854\n",
      "lovely 0.8106936812400818\n",
      "stunningly_beautiful 0.7329413294792175\n",
      "breathtakingly_beautiful 0.7231340408325195\n",
      "wonderful 0.6854086518287659\n",
      "fabulous 0.6700063943862915\n",
      "loveliest 0.6612576246261597\n",
      "prettiest 0.6595001816749573\n",
      "beatiful 0.6593326330184937\n",
      "magnificent 0.6591402888298035\n"
     ]
    }
   ],
   "source": [
    "for x, y in wv.most_similar('beautiful'):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for word in ['car', 'minivan', 'bicycle', 'airplane']:\n",
    "    vectors.append(wv.get_vector(word))\n",
    "V = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = V.mean(axis=0)\n",
    "#v = v - wv.get_vector('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 0.852258026599884),\n",
       " ('minivan', 0.8156529664993286),\n",
       " ('vehicle', 0.7754934430122375),\n",
       " ('SUV', 0.7660486698150635),\n",
       " ('bicycle', 0.7264742255210876),\n",
       " ('pickup_truck', 0.723552942276001),\n",
       " ('scooter', 0.7198848724365234),\n",
       " ('truck', 0.7041884064674377),\n",
       " ('Jeep', 0.7000145316123962),\n",
       " ('motorcycle', 0.6802986264228821)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy\n",
    "\n",
    "FRANCE : PARIS = ITALY : ?\n",
    "\n",
    "PARIS - FRANCE + ITALY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Milan', 0.7222141623497009),\n",
       " ('Rome', 0.702830970287323),\n",
       " ('Palermo_Sicily', 0.5967570543289185),\n",
       " ('Italian', 0.5911272764205933),\n",
       " ('Tuscany', 0.5632812976837158),\n",
       " ('Bologna', 0.5608358383178711),\n",
       " ('Sicily', 0.5596384406089783),\n",
       " ('Bologna_Italy', 0.5470058917999268),\n",
       " ('Berna_Milan', 0.5464027523994446),\n",
       " ('Genoa', 0.5308900475502014)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['Paris', 'Italy'], negative=['France'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Queen', 0.4929387867450714),\n",
       " ('Tupou_V.', 0.45174285769462585),\n",
       " ('Oprah_BFF_Gayle', 0.4422132968902588),\n",
       " ('Jackson', 0.440250426530838),\n",
       " ('NECN_Alison', 0.4331282675266266),\n",
       " ('Whitfield', 0.42834725975990295),\n",
       " ('Ida_Vandross', 0.42084527015686035),\n",
       " ('prosecutor_Dan_Satterberg', 0.420758992433548),\n",
       " ('martin_Luther_King', 0.42059651017189026),\n",
       " ('Coretta_King', 0.4202733635902405)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['King', 'Woman'], negative=['Man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.doesnt_match(\"school professor apple student\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = wv['school']\n",
    "vr = wv['professor']\n",
    "vx = wv['student']\n",
    "m = (vp + vr + vx) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similar_by_vector(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('lecturer', 'school'),\n",
    "    ('lecturer', 'professor'),\n",
    "    ('lecturer', 'student'),\n",
    "    ('lecturer', 'teacher'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.similarity('buy', 'money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a global model for YELP reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_file = '../lexicon/data/yelp_sample.json'\n",
    "with open(review_data_file, 'r') as infile:\n",
    "    R = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[x.lower() for x in word_tokenize(doc['content']) if x not in punctuation] for doc in R]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R0 = gensim.models.Word2Vec(sentences=data, epochs=25, window=6, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R0.wv.most_similar('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application example: use graph community detection to find aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sim = 0.7\n",
    "G = nx.Graph()\n",
    "for word in tqdm(R0.wv.index_to_key):\n",
    "    for match, sim in R0.wv.most_similar(word):\n",
    "        if sim >= min_sim:\n",
    "            G.add_edge(word, match, sim=sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b, c in G.edges(data=True):\n",
    "    print(a, b, c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = Network('1500px', '1500px')\n",
    "nt.from_nx(G.subgraph(list(G.nodes)[:100]))\n",
    "nt.show('word2vec.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = greedy_modularity_communities(G)\n",
    "for community in communities:\n",
    "    print(list(community)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update an existing model\n",
    "Let's create a collection for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = defaultdict(list)\n",
    "for review in R:\n",
    "    content, categories = review['content'], review['categories']\n",
    "    tokens = [x.lower() for x in word_tokenize(content) if x not in punctuation]\n",
    "    for category in categories:\n",
    "        corpora[category].append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora['RV Rental'][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the global model with the local information (i.e., create a model for each category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_categories = ['Burgers', 'Indian', 'Italian', 'Seafood']\n",
    "models = {}\n",
    "runs = [(c, d) for c, d in corpora.items() if c in selected_categories]\n",
    "for category, data in tqdm(runs):\n",
    "    models[category] = copy.deepcopy(R0)\n",
    "    models[category].train(data, total_examples=R0.corpus_count, epochs=R0.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'food'\n",
    "for cat, m in models.items():\n",
    "    print(cat, [x for x, y in m.wv.most_similar(word, topn=20)][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = defaultdict(lambda: 0)\n",
    "for cat, m in models.items():\n",
    "    for x, y in m.wv.most_similar(word, topn=20):\n",
    "        counter[x] += 1\n",
    "for cat, m in models.items():\n",
    "    print(cat, [x for x, y in m.wv.most_similar(word, topn=20) if counter[x] < 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: train a model from wordnet\n",
    "\n",
    "How `word2vec` may be used to disambiguate words and lookup for synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "words = ['cat', 'dog', 'bird', 'fish']\n",
    "\n",
    "h = lambda s: s.hypernyms()\n",
    "p = lambda s: s.hyponyms()\n",
    "\n",
    "def get_pseudo_sentences(word, context=3):\n",
    "    sentences = []\n",
    "    for s in wn.synsets(word):\n",
    "        for lemma in s.lemmas():\n",
    "            sentences.append([lemma.name(), s.name()])\n",
    "        for i, j in enumerate(s.closure(h)):\n",
    "            sentences.append([s.name(), j.name()])\n",
    "            for lemma in j.lemmas():\n",
    "                sentences.append([lemma.name(), j.name()])\n",
    "            if i == context:\n",
    "                break\n",
    "        for i, j in enumerate(s.closure(p)):\n",
    "            sentences.append([j.name(), s.name()])\n",
    "            for lemma in j.lemmas():\n",
    "                sentences.append([lemma.name(), j.name()])\n",
    "            if i == context:\n",
    "                break\n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "for w in words:\n",
    "    sentences += get_pseudo_sentences(w)\n",
    "    \n",
    "print(sentences[0])\n",
    "\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.wv.most_similar('fish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
