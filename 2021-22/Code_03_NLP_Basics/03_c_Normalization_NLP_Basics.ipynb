{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'first', 'met', 'him', 'he', 'was', \"n't\", 'very', 'quiet', '.', 'He', 'spoke', 'without', 'stopping', 'during', 'the', 'entire', 'two', 'hour', 'long', 'journey', 'from', 'Washington', 'to', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"When I first met him he wasn't very quiet. He spoke without stopping during the entire two hour long journey from Washington to New York.\"\n",
    "\n",
    "nltk_word = word_tokenize(sentence)\n",
    "\n",
    "print(nltk_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When I', 'first', 'met', 'him', 'he', 'was', \"n't\", 'very', 'quiet', '.', 'He', 'spoke', 'without', 'stopping', 'during', 'the', 'entire', 'two', 'hour', 'long', 'journey', 'from', 'Washington', 'to', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "whiteList = \"new\"\n",
    "\n",
    "if whiteList in nltk_word:\n",
    "    whiteIndex = nltk_word.index(whiteList)\n",
    "\n",
    "nltk_word[whiteIndex:whiteIndex+2] = [' '.join(nltk_word[whiteIndex:whiteIndex+2])]\n",
    "print(nltk_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When I', 'first', 'met', \"n't\", 'quiet', 'He', 'spoke', 'without', 'stopping', 'entire', 'two', 'hour', 'long', 'journey', 'Washington', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "# Remove punctation and stopwords\n",
    "\n",
    "import string\n",
    "#   import nltk\n",
    "#   nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_en = stopwords.words(\"english\")\n",
    "\n",
    "filteredWords = [w for w in nltk_word if (w not in list(string.punctuation)) and (w not in stop_words_en)]\n",
    "\n",
    "print(filteredWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'i', 'first', 'met', 'him', 'he', 'was', \"n't\", 'very', 'quiet', '.', 'he', 'spoke', 'without', 'stopping', 'during', 'the', 'entire', 'two', 'hour', 'long', 'journey', 'from', 'washington', 'to', 'new york', '.']\n"
     ]
    }
   ],
   "source": [
    "# Converting to lowercase\n",
    "\n",
    "lowerWords = [w.lower() for w in nltk_word]\n",
    "\n",
    "print(lowerWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023049\n"
     ]
    }
   ],
   "source": [
    "# Number words to numeric\n",
    "\n",
    "from word2number import w2n\n",
    "\n",
    "text = \"two million twenty three thousand and forty nine\"\n",
    "print(w2n.word_to_num(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nino\n"
     ]
    }
   ],
   "source": [
    "# unicode characters\n",
    "\n",
    "import unidecode\n",
    "\n",
    "test_word = unidecode.unidecode(\"ni√±o\")\n",
    "print(test_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'i', 'first', 'met', 'him', 'he', 'wa', \"n't\", 'veri', 'quiet', '.', 'he', 'spoke', 'without', 'stop', 'dure', 'the', 'entir', 'two', 'hour', 'long', 'journey', 'from', 'washington', 'to', 'new york', '.']\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "porterWords = [porter.stem(w) for w in nltk_word]\n",
    "\n",
    "print(porterWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multidimension', 'come', 'sing', 'stem']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"multidimensional\", \"coming\", \"sing\", \"stemming\" ]\n",
    "\n",
    "[porter_stemmer.stem(w) for w in words]\n",
    "\n",
    "#   Porter Stemming Algorithm   - http://snowball.tartarus.org/algorithms/porter/stemmer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'i', 'first', 'met', 'him', 'he', 'was', \"n't\", 'veri', 'quiet', '.', 'he', 'spoke', 'without', 'stop', 'dure', 'the', 'entir', 'two', 'hour', 'long', 'journey', 'from', 'washington', 'to', 'new york', '.']\n"
     ]
    }
   ],
   "source": [
    "# SnowballStemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "snowballWords = [snowball.stem(w) for w in nltk_word]\n",
    "\n",
    "print(snowballWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'i', 'first', 'met', 'him', 'he', 'was', \"n't\", 'very', 'quiet', '.', 'he', 'spok', 'without', 'stop', 'dur', 'the', 'entir', 'two', 'hour', 'long', 'journey', 'from', 'washington', 'to', 'new york', '.']\n"
     ]
    }
   ],
   "source": [
    "# LancasterStemmer\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "lancasterWords = [lancaster.stem(w) for w in nltk_word]\n",
    "\n",
    "print(lancasterWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'first', 'met', 'him', 'he', 'was', \"n't\", 'very', 'quiet', '.', 'He', 'spoke', 'without', 'stopp', 'dur', 'the', 'entire', 'two', 'hour', 'long', 'journey', 'from', 'Washton', 'to', 'New York', '.']\n"
     ]
    }
   ],
   "source": [
    "# RegexpStemmer\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "regexp = RegexpStemmer('ing')\n",
    "\n",
    "regexpWords = [regexp.stem(w) for w in nltk_word]\n",
    "\n",
    "print(regexpWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'first', 'met', 'him', 'he', 'wa', \"n't\", 'very', 'quiet', '.', 'He', 'spoke', 'without', 'stopping', 'during', 'the', 'entire', 'two', 'hour', 'long', 'journey', 'from', 'Washington', 'to', 'New York', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize as noun (default)   :   Valid options are `\"n\"` for nouns, `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"` for satellite adjectives.\n",
    "\n",
    "#   import nltk\n",
    "#   nltk.download('wordnet')\n",
    "#   nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "lemmaWords = [lemma.lemmatize(w) for w in nltk_word]\n",
    "\n",
    "print(lemmaWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'first', 'meet', 'him', 'he', 'be', \"n't\", 'very', 'quiet', '.', 'He', 'speak', 'without', 'stop', 'during', 'the', 'entire', 'two', 'hour', 'long', 'journey', 'from', 'Washington', 'to', 'New York', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize as verbs\n",
    "\n",
    "lemmaWords = [lemma.lemmatize(w, pos = 'v') for w in nltk_word]\n",
    "\n",
    "print(lemmaWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize as adjectives\n",
    "\n",
    "lemma.lemmatize('better', pos = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize as adverbs\n",
    "\n",
    "lemma.lemmatize('better', pos = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'I', 'first', 'meet', 'he', 'he', 'be', 'not', 'very', 'quiet', '.', 'he', 'speak', 'without', 'stop', 'during', 'the', 'entire', 'two', 'hour', 'long', 'journey', 'from', 'Washington', 'to', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#spaCy determines the part-of-speech tag by default and assigns the corresponding lemma. It comes with a bunch of prebuilt models where the 'en' we just downloaded above is one of the standard ones for english.\n",
    "\n",
    "sentence = ' '.join(nltk_word)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'be', 'come', 'in', 'a', 'multimensional']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I'm coming in a multimensional\")\n",
    "\n",
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'I', 'first', 'meet', 'not', 'quiet', 'he', 'speak', 'without', 'stop', 'entire', 'two', 'hour', 'long', 'journey', 'Washington', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\" \".join(filteredWords))\n",
    "\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfb75dc7d96d332aeaa21ae2d7b833083e393049992869c7fba312a269a18e76"
  },
  "kernelspec": {
   "display_name": "text-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
