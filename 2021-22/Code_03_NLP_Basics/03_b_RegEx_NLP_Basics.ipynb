{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex module\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The findall() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a list of all matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Print a list of all matches:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.findall(\"ai\", string)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list contains the matches in the order they are found.\n",
    "\n",
    "If no matches are found, an empty list is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Return an empty list if no match was found:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.findall(\"Portugal\", string)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The search() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search() function searches the string for a match, and returns a Match object if there is a match.\n",
    "\n",
    "If there is more than one match, only the first occurrence of the match will be returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first white-space character is located in position: 3\n"
     ]
    }
   ],
   "source": [
    "# Search for the first white-space character in the string:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.search(\"\\s\", string)\n",
    "print(\"The first white-space character is located in position:\", x.start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no matches are found, the value None is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Make a search that returns no match:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.search(\"Portugal\", string)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The split() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split() function returns a list where the string has been split at each match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'rain', 'in', 'Spain']\n"
     ]
    }
   ],
   "source": [
    "# Split at each white-space character:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.split(\"\\s\", string)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control the number of occurrences by specifying the maxsplit parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'rain in Spain']\n"
     ]
    }
   ],
   "source": [
    "# Split the string only at the first occurrence:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.split(\"\\s\", string, 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sub() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sub() function replaces the matches with the text of your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The9rain9in9Spain\n"
     ]
    }
   ],
   "source": [
    "# Replace every white-space character with the number 9:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.sub(\"\\s\", \"9\", string)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control the number of replacements by specifying the count parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The9rain9in Spain\n"
     ]
    }
   ],
   "source": [
    "# Replace the first 2 occurrences:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.sub(\"\\s\", \"9\", string, 2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Match Object is an object containing information about the search and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(5, 7), match='ai'>\n",
      "(5, 7)\n",
      "The rain in Spain\n",
      "ai\n"
     ]
    }
   ],
   "source": [
    "# Do a search that will return a Match Object:\n",
    "string = \"The rain in Spain\"\n",
    "x = re.search(\"ai\", string)\n",
    "\n",
    "print(x)            # the object\n",
    "print(x.span())     # a tuple containing the start-, and end positions of the match.\n",
    "print(x.string)     # the string passed into the function\n",
    "print(x.group())    # the part of the string where there was a match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi maybe there is some mistake'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can combine and create a function\n",
    "\n",
    "def cleanText(text: str = \"\") -> str:\n",
    "    sentence = str(text)\n",
    "    #Removes unicode strings like \"\\u002c\" and \"x96\"        \n",
    "    sentence = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', sentence)\n",
    "    sentence = re.sub(r'[^\\x00-\\x7f]',r'',sentence)        \n",
    "    #remove exclamation & question marks\n",
    "    sentence = re.sub(r\"(\\!)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\?)+\", ' ', sentence)   \n",
    "    #remove punctuation\n",
    "    sentence = re.sub(r\"(\\,)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\;)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\:)+\", ' ', sentence)\n",
    "    #remove parentheses\n",
    "    sentence = re.sub(r\"(\\()+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\))+\", ' ', sentence)\n",
    "    #remove underscore\n",
    "    sentence = re.sub(r\"(\\_)+\", ' ', sentence)    \n",
    "    #remove other simbols        \n",
    "    sentence = re.sub(r\"(\\-)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\+)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\/)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\*)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\')+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\\")+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\#)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\>)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\<)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\£)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\$)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\%)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\&)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\€)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\=)+\", ' ', sentence)\n",
    "    sentence = re.sub(r\"(\\^)+\", ' ', sentence)\n",
    "    #trim\n",
    "    sentence = sentence.strip('\\'\"')\n",
    "    #Remove additional white spaces\n",
    "    sentence = re.sub('[\\s]+', ' ', sentence)\n",
    "    sentence = re.sub('[\\n]+', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    text = sentence\n",
    "    return text\n",
    "\n",
    "randomSentence = \"Hi,   maybe  there$is some::::::::mistake!!\"\n",
    "\n",
    "clearSentence = cleanText(randomSentence)\n",
    "clearSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK RegEx tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantate the RegexpTokenizer class or use the simple helper function regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Characters', 'like', 'periods', 'exclamation', 'point', 'and', 'newline', 'char', 'are', 'used', 'to', 'separate', 'the', 'sentences', 'But', 'one', 'drawback', 'with', 'split', 'method', 'that', 'we', 'can', 'only', 'use', 'one', 'separator', 'at', 'a', 'time', 'So', 'sentence', 'tokenization', \"won't\", 'be', 'foolproof', 'with', 'split', 'method']\n"
     ]
    }
   ],
   "source": [
    "para = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tokenization won't be foolproof with split() method.\"\"\"\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer   #   Source code : https://www.nltk.org/_modules/nltk/tokenize/regexp.html\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\", gaps=False, discard_empty=True)\n",
    "\n",
    "print(tokenizer.tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Characters', 'like', 'periods', 'exclamation', 'point', 'and', 'newline', 'char', 'are', 'used', 'to', 'separate', 'the', 'sentences', 'But', 'one', 'drawback', 'with', 'split', 'method', 'that', 'we', 'can', 'only', 'use', 'one', 'separator', 'at', 'a', 'time', 'So', 'sentence', 'tokenization', \"won't\", 'be', 'foolproof', 'with', 'split', 'method']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(para, \"[\\w']+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112310\n",
      "[The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Flauius, Murellus, and certaine Commoners ouer the Stage.\n",
      "\n",
      "  Flauius. Hence: home you idle Creatures, get you home:\n",
      "Is this a Holiday? What, know you not\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('gutenberg')\n",
    "\n",
    "shakespeare_caesar = nltk.corpus.gutenberg.raw('shakespeare-caesar.txt')\n",
    "\n",
    "print(len(shakespeare_caesar))\n",
    "\n",
    "print(shakespeare_caesar[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', '1599]', 'Actus', 'Primus.', 'Scoena', 'Prima.', 'Enter', 'Flauius,', 'Murellus,', 'and', 'certaine', 'Commoners', 'ouer', 'the', 'Stage.', 'Flauius.', 'Hence:', 'home', 'you', 'idle', 'Creatures,', 'get', 'you', 'home:', 'Is', 'this', 'a', 'Holiday?', 'What,', 'know', 'you', 'not', '(Being', 'Mechanicall)', 'you', 'ought', 'not', 'walke', 'Vpon', 'a', 'labouring', 'day,', 'without']\n",
      "N° of split words : 20459\n",
      "N° of unique split words : 4992\n"
     ]
    }
   ],
   "source": [
    "splitCaesar = shakespeare_caesar.split()\n",
    "\n",
    "print(splitCaesar[:50])\n",
    "\n",
    "print(f\"N° of split words : {len(splitCaesar)}\")\n",
    "\n",
    "print(f\"N° of unique split words : {len(set(splitCaesar))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', 'Actus', 'Primus', 'Scoena', 'Prima', 'Enter', 'Flauius', 'Murellus', 'and', 'certaine', 'Commoners', 'ouer', 'the', 'Stage', 'Flauius', 'Hence', 'home', 'you', 'idle', 'Creatures', 'get', 'you', 'home', 'Is', 'this', 'a', 'Holiday', 'What', 'know', 'you', 'not', 'Being', 'Mechanicall', 'you', 'ought', 'not', 'walke', 'Vpon', 'a', 'labouring', 'day', 'without', 'the']\n",
      "N° of regex words : 20804\n",
      "N° of unique regex words : 3543\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "regexCaesar = regexp_tokenize(shakespeare_caesar, r'[A-Za-z]+', gaps=False)\n",
    "\n",
    "print(regexCaesar[:50])\n",
    "\n",
    "print(f\"N° of regex words : {len(regexCaesar)}\")\n",
    "\n",
    "print(f\"N° of unique regex words : {len(set(regexCaesar))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Enter', 'Flauius', ',', 'Murellus', ',', 'and', 'certaine', 'Commoners', 'ouer', 'the', 'Stage', '.', 'Flauius', '.', 'Hence', ':', 'home', 'you', 'idle', 'Creatures', ',', 'get', 'you', 'home', ':', 'Is', 'this', 'a', 'Holiday', '?', 'What', ',', 'know']\n",
      "N° of token words : 25251\n",
      "N° of unique token words : 3610\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wTokenCaesar = word_tokenize(shakespeare_caesar, preserve_line=False)\n",
    "\n",
    "print(wTokenCaesar[:50])\n",
    "\n",
    "print(f\"N° of token words : {len(wTokenCaesar)}\")\n",
    "\n",
    "print(f\"N° of unique token words : {len(set(wTokenCaesar))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfb75dc7d96d332aeaa21ae2d7b833083e393049992869c7fba312a269a18e76"
  },
  "kernelspec": {
   "display_name": "text-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
