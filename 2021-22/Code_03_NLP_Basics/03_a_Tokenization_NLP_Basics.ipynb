{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " \"But one drawback with split() method, that we can only use one separator at a time! So sentence tokenization won't be foolproof with split() method.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import (sent_tokenize, word_tokenize)\n",
    "\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "para = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tokenization won't be foolproof with split() method.\"\"\"\n",
    "\n",
    "para.split(\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get 3 tokens (sentences) in this paragraph\n",
      "['Characters like periods, exclamation point and newline char are used to separate the sentences.', 'But one drawback with split() method, that we can only use one separator at a time!', \"So sentence tokenization won't be foolproof with split() method.\"]\n"
     ]
    }
   ],
   "source": [
    "#   Tokenizing text into sentences  : Return a sentence-tokenized copy of text, using NLTK’s recommended sentence tokenizer (currently PunktSentenceTokenizer for the specified language).\n",
    "nltk_sent_para = sent_tokenize(para, language='english')\n",
    "\n",
    "print(f\"We get {len(nltk_sent_para)} tokens (sentences) in this paragraph\")\n",
    "\n",
    "print(nltk_sent_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code\n",
    "\n",
    "```python\n",
    "def sent_tokenize(text, language=\"english\"):\n",
    "    \"\"\"\n",
    "    Return a sentence-tokenized copy of *text*,\n",
    "    using NLTK's recommended sentence tokenizer\n",
    "    (currently :class:`.PunktSentenceTokenizer`\n",
    "    for the specified language).\n",
    "\n",
    "    :param text: text to split into sentences\n",
    "    :param language: the model name in the Punkt corpus\n",
    "    \"\"\"\n",
    "    tokenizer = load(f\"tokenizers/punkt/{language}.pickle\")\n",
    "    return tokenizer.tokenize(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module. This instance has already been trained and works well for many European languages. So it knows what punctuation and characters mark the end of a sentence and the beginning of a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get 49 tokens (words) in this paragraph\n",
      "['Characters', 'like', 'periods', ',', 'exclamation', 'point', 'and', 'newline', 'char', 'are', 'used', 'to', 'separate', 'the', 'sentences', '.', 'But', 'one', 'drawback', 'with', 'split', '(', ')', 'method', ',', 'that', 'we', 'can', 'only', 'use', 'one', 'separator', 'at', 'a', 'time', '!', 'So', 'sentence', 'tokenization', 'wo', \"n't\", 'be', 'foolproof', 'with', 'split', '(', ')', 'method', '.']\n"
     ]
    }
   ],
   "source": [
    "#   Tokenizing text into words  : Return a tokenized copy of text, using NLTK’s recommended word tokenizer (currently an improved TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\n",
    "nltk_word_para = word_tokenize(para, language=\"english\", preserve_line=False)\n",
    "\n",
    "print(f\"We get {len(nltk_word_para)} tokens (words) in this paragraph\")\n",
    "\n",
    "print(nltk_word_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code\n",
    "\n",
    "```python\n",
    "from nltk.tokenize.destructive import NLTKWordTokenizer\n",
    "\n",
    "_treebank_word_tokenizer = NLTKWordTokenizer()\n",
    "\n",
    "def word_tokenize(text, language=\"english\", preserve_line=False):\n",
    "    \"\"\"\n",
    "    Return a tokenized copy of *text*,\n",
    "    using NLTK's recommended word tokenizer\n",
    "    (currently an improved :class:`.TreebankWordTokenizer`\n",
    "    along with :class:`.PunktSentenceTokenizer`\n",
    "    for the specified language).\n",
    "\n",
    "    :param text: text to split into words\n",
    "    :type text: str\n",
    "    :param language: the model name in the Punkt corpus\n",
    "    :type language: str\n",
    "    :param preserve_line: A flag to decide whether to sentence tokenize the text or not.\n",
    "    :type preserve_line: bool\n",
    "    \"\"\"\n",
    "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
    "    return [\n",
    "        token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word_tokenize() function is a wrapper function that calls tokenize() on an instance of the TreebankWordTokenizer class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tokenization won't be foolproof with split() method."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#   nlp = spacy.load(\"en_core_web_md\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp(para)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get 3 tokens (sentences) in this paragraph\n",
      "['Characters like periods, exclamation point and newline char are used to separate the sentences.', 'But one drawback with split() method, that we can only use one separator at a time!', \"So sentence tokenization won't be foolproof with split() method.\"]\n"
     ]
    }
   ],
   "source": [
    "#   Tokenizing text into sentences\n",
    "\n",
    "spacy_sent_para = [sent.text for sent in doc.sents]\n",
    "\n",
    "print(f\"We get {len(spacy_sent_para)} tokens (sentences) in this paragraph\")\n",
    "\n",
    "print(spacy_sent_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get 49 tokens (words) in this paragraph\n",
      "['Characters', 'like', 'periods', ',', 'exclamation', 'point', 'and', 'newline', 'char', 'are', 'used', 'to', 'separate', 'the', 'sentences', '.', 'But', 'one', 'drawback', 'with', 'split', '(', ')', 'method', ',', 'that', 'we', 'can', 'only', 'use', 'one', 'separator', 'at', 'a', 'time', '!', 'So', 'sentence', 'tokenization', 'wo', \"n't\", 'be', 'foolproof', 'with', 'split', '(', ')', 'method', '.']\n"
     ]
    }
   ],
   "source": [
    "#   Tokenizing text into words\n",
    "\n",
    "spacy_word_para = [word.text for word in doc]\n",
    "\n",
    "print(f\"We get {len(spacy_word_para)} tokens (words) in this paragraph\")\n",
    "\n",
    "print(spacy_word_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'separate'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://spacy.io/api/token#attributes\n",
    "doc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfb75dc7d96d332aeaa21ae2d7b833083e393049992869c7fba312a269a18e76"
  },
  "kernelspec": {
   "display_name": "text-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
